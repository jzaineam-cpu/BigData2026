{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxZ2Kqz6kuivX/x6c9kfkH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazaineam1/BigData2026/blob/main/Cuadernos/4_ETL_ELT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b847a1c3"
      },
      "source": [
        "# Sesión 5: Implementación de ETL mediante herramientas de flujos de trabajo\n",
        "\n",
        "## Objetivos de la Sesión:\n",
        "* Comparar los enfoques ETL (Extract, Transform, Load) y ELT (Extract, Load, Transform).\n",
        "* Resolver un caso práctico de ingeniería de datos utilizando Python para construir un pipeline ETL/ELT.\n",
        "* Entender conceptos clave como  ingesta de datos y herramientas de orquestación.\n",
        "* Identificar y citar fuentes relevantes para el estudio de ETL/ELT.\n",
        "\n",
        "---\n",
        "\n",
        "## Bloque 1: Reto Estratégico y Arquitectura Conceptual (40 min)\n",
        "\n",
        "### Reto Práctico: Consolidación de Datos Financieros Universitarios\n",
        "\n",
        "Una universidad recibe **mensualmente tres archivos CSV** que contienen información financiera de sus estudiantes (por ejemplo, ingresos, gastos, becas, etc.).\n",
        "\n",
        "**Características de los archivos:**\n",
        "*   Pueden contener **registros duplicados**.\n",
        "*   Pueden presentar **valores nulos** o errores en los datos.\n",
        "*   El objetivo es **consolidar históricamente** estos datos para análisis financieros y de rendimiento estudiantil.\n",
        "\n",
        "**Restricción Clave:**\n",
        "*   **No se puede modificar el sistema de origen** de los archivos.\n",
        "\n",
        "### Preguntas Iniciales para Discusión:\n",
        "\n",
        "Para abordar este reto, es fundamental plantearse las siguientes preguntas que nos guiarán hacia el diseño de una arquitectura de datos robusta:\n",
        "\n",
        "1.  **¿Dónde almacenar los datos crudos originales?**\n",
        "\n",
        "2.  **Si llega un archivo corrupto, ¿qué hacemos?**\n",
        "\n",
        "3.  **¿Cómo evitamos duplicados al procesar múltiples veces los mismos archivos? (Concepto de idempotencia).**\n",
        "  \n",
        "4.  **¿Dónde implementamos las reglas de negocio (cálculos, transformaciones)?**\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Crear carpeta staging\n",
        "os.makedirs(\"staging\", exist_ok=True)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def generar_datos_mes(mes, anio=2024, n_registros=100000):\n",
        "\n",
        "    ids = np.random.randint(1, 3000, n_registros)  # Puede generar duplicados\n",
        "    # Inicializar salarios y gastos como float para poder asignar None (que se convierte a NaN)\n",
        "    salarios = np.random.randint(-1000, 8000, n_registros).astype(float)  # Incluye negativos\n",
        "    gastos = np.random.randint(-500, 5000, n_registros).astype(float)     # Incluye negativos\n",
        "\n",
        "    # Fechas con formatos inconsistentes\n",
        "    fechas = []\n",
        "    for _ in range(n_registros):\n",
        "        fecha = datetime(anio, mes, random.randint(1, 28))\n",
        "        formato = random.choice([\n",
        "            \"%Y-%m-%d\",\n",
        "            \"%d/%m/%Y\",\n",
        "            \"%m-%d-%Y\"\n",
        "        ])\n",
        "        fechas.append(fecha.strftime(formato))\n",
        "\n",
        "    # Correos (PII)\n",
        "    correos = [f\"usuario{random.randint(1,5000)}@universidad.edu\" for _ in range(n_registros)]\n",
        "\n",
        "    # Introducir valores nulos aleatorios\n",
        "    salarios[np.random.choice(n_registros, size=300, replace=False)] = None\n",
        "    gastos[np.random.choice(n_registros, size=200, replace=False)] = None\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"id\": ids,\n",
        "        \"salario\": salarios,\n",
        "        \"gastos\": gastos,\n",
        "        \"fecha\": fechas,\n",
        "        \"correo\": correos\n",
        "    })\n",
        "\n",
        "    # Introducir filas completamente corruptas\n",
        "    for _ in range(10):\n",
        "        df.loc[random.randint(0, n_registros-1)] = [None, None, None, \"fecha_invalida\", \"correo_invalido\"]\n",
        "\n",
        "    # Introducir duplicados completos\n",
        "    df = pd.concat([df, df.sample(100)], ignore_index=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Generar archivos mensuales\n",
        "for mes in [1, 2, 3, 4, 5, 6, 7]:\n",
        "    df_mes = generar_datos_mes(mes)\n",
        "    df_mes.to_csv(f\"staging/finanzas_mes_{mes}.csv\", index=False)\n",
        "\n",
        "print(\"Archivos problemáticos generados en carpeta 'staging/'\")"
      ],
      "metadata": {
        "id": "x2EhQsJNzegK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kV9HbGHm5EGF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "166c6d63"
      },
      "source": [
        "## Bloque 3: ETL clásico (Extracción, Transformación, Carga)\n",
        "\n",
        "El modelo ETL clásico implica transformar los datos antes de cargarlos al almacén. Los pasos son:\n",
        "\n",
        "*   **Extracción (Extract)**\n",
        "En esta primera etapa, se recolectan datos brutos de múltiples orígenes, que pueden incluir bases de datos relacionales (SQL), sistemas CRM, archivos planos (como CSV), APIs, sensores de IoT o aplicaciones SaaS. Los datos extraídos suelen colocarse inicialmente en un área de preparación temporal conocida como staging area\n",
        "*  **Transformación (Transform)**\n",
        "Aquí, los datos extraídos se someten a una serie de reglas de negocio y procesos de limpieza en un servidor de procesamiento secundario antes de ser movidos al sistema de destino. Las tareas principales incluyen:\n",
        ">* Limpieza y validación: Se eliminan datos inconsistentes, se manejan valores nulos y se corrigen errores de formato.\n",
        ">* Estandarización: Se unifican formatos, como fechas o unidades de medida.\n",
        ">* Deduplicación: Se identifican y eliminan registros repetidos.\n",
        ">* Seguridad y cumplimiento: Se aplican técnicas de enmascaramiento o anonimización de información personal identificable (PII) para cumplir con regulaciones como GDPR o HIPAA antes de que el dato toque el almacenamiento final.\n",
        "> * Derivación y enriquecimiento: Construcción de nuevas variables, métricas y atributos que representan la lógica empresarial y permiten análisis posteriores.\n",
        "*    Carga (Load)\n",
        "En el paso final, los datos ya transformados, estructurados y limpios se guardan en el sistema de destino (como un Data Warehouse OLAP). Una vez cargados, los datos están listos para ser utilizados directamente por herramientas de Inteligencia de Negocios (BI), analistas y científicos de datos para generar informes y conocimientos estratégicos\n",
        "\n",
        "### Características del ETL Clásico\n",
        "*  Esquema de escritura (Schema-on-Write): Requiere que la estructura y el esquema del destino se definan meticulosamente antes de cargar los datos.\n",
        "* Enfoque en TI: Tradicionalmente, estos procesos han sido diseñados y gestionados por equipos especializados de ingeniería de datos o departamentos de TI.\n",
        "* Procesamiento por lotes: Por lo general, se ejecuta en intervalos programados (diarios, semanales), aunque existen implementaciones en tiempo real.\n",
        "* Herramientas comunes: Algunas de las plataformas más conocidas para este flujo son Informatica PowerCenter, Talend y Microsoft SSIS.\n",
        "\n",
        "### Implementación en Python\n",
        "\n",
        "Utilizaremos `pandas` para procesar los CSV y una base de datos `SQLite` como almacén simulado. El siguiente código ejecuta un pipeline ETL:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracción"
      ],
      "metadata": {
        "id": "D47dev9_aT4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "archivos = [f for f in os.listdir('staging') if f.endswith('.csv')]\n",
        "\n",
        "dataframes = []\n",
        "\n",
        "for fname in archivos:\n",
        "    df = pd.read_csv(os.path.join('staging', fname))\n",
        "    print(f\"Archivo {fname} leído con {len(df)} registros\")\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Unificamos todos los meses\n",
        "df_total = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "print(f\"Total registros extraídos: {len(df_total)}\")"
      ],
      "metadata": {
        "id": "lyviAv9FaRYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total.head()"
      ],
      "metadata": {
        "id": "SHN2qn13aacF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El dato pasa de disco a memoria.\n",
        "\n",
        "* Aún no hay reglas de negocio.\n",
        "\n",
        "* Solo estamos recolectando."
      ],
      "metadata": {
        "id": "4v5xuyU4aiA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformación\n",
        "1. Tratamiento de valores nulos\n",
        "\n",
        "Durante la transformación, existen varias formas de abordar los datos faltantes:\n",
        "\n",
        "*  Imputación: Consiste en rellenar los valores nulos con datos específicos para no perder la fila completa. Puede optar por valores predeterminados como \"0\" para campos numéricos, o etiquetas como \"Desconocido\" o \"Otros\" para categorías. También es común usar métodos estadísticos como imputar el promedio o la moda, pero depende de las reglas de negocio.\n",
        "* Filtrado o Eliminación: Si un campo crítico (como un ID de cliente o una fecha de transacción) es nulo, la práctica común es descartar la fila para evitar análisis sesgados.\n",
        "* Validación de Calidad: Implementar pruebas de nulos (como el test not_null en dbt) permite marcar registros obligatorios que faltan y generar alertas antes de que los datos lleguen a los informes finales."
      ],
      "metadata": {
        "id": "mU9LdvlQapUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Campos críticos: id y fecha**: Es una regla estructural para este caso, si falta identidad o temporalidad, el registro no es analizable."
      ],
      "metadata": {
        "id": "sgVmrLxafdJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_total.isna().any(axis=0)"
      ],
      "metadata": {
        "id": "fW2m8z1ZiTG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "registros_antes = len(df_total)\n",
        "df_total = df_total.dropna(subset=['id', 'fecha'])\n",
        "print(\"Registros eliminados por id/fecha nulos:\", registros_antes - len(df_total))"
      ],
      "metadata": {
        "id": "KnMX57R4auej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La imputación masiva puede introducir ruido o sesgos artificiales en los modelos de aprendizaje automático si no se realiza con cuidado, alterando las líneas de ajuste y la exactitud de los resultados, para nuestro ejercico tener gastos nulos indicará que tienen valor 0.\n",
        "\n",
        "Más información  en  Rubin, D. B. (1976). Inference and missing data. Biometrika, 63(3), 581–590."
      ],
      "metadata": {
        "id": "sWXNkoFMg2_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_total"
      ],
      "metadata": {
        "id": "u9oTZzMSpAi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total['gastos'].isna().sum()"
      ],
      "metadata": {
        "id": "yfUcUop7g9Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total[\"gastos\"] = df_total[\"gastos\"].fillna(0)"
      ],
      "metadata": {
        "id": "Mi9Py67tg41F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total['gastos'].isna().sum()"
      ],
      "metadata": {
        "id": "Mb6gtJQVnKjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Validar reglas básicas"
      ],
      "metadata": {
        "id": "aPLcScqqa-Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_total = df_total[df_total['salario'] > 0]\n",
        "df_total = df_total[df_total['gastos'] >= 0]"
      ],
      "metadata": {
        "id": "umWaQcx8bE3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* No puede haber salario negativo.\n",
        "\n",
        "* Gastos no pueden ser negativos.\n",
        "3. Estandarización\n"
      ],
      "metadata": {
        "id": "OCxSj4dzbIqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def estandarizar_fecha(valor):\n",
        "    if pd.isnull(valor):\n",
        "        return pd.NaT\n",
        "\n",
        "    valor = str(valor).strip()\n",
        "\n",
        "    # Caso 1: ISO estándar YYYY-MM-DD\n",
        "    try:\n",
        "        if \"-\" in valor and len(valor.split(\"-\")[0]) == 4:\n",
        "            return pd.to_datetime(valor, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Caso 2: Formato europeo DD/MM/YYYY\n",
        "    if \"/\" in valor:\n",
        "        try:\n",
        "            return pd.to_datetime(valor, format=\"%d/%m/%Y\", errors=\"coerce\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Caso 3: Formato americano MM-DD-YYYY\n",
        "    if \"-\" in valor:\n",
        "        try:\n",
        "            return pd.to_datetime(valor, format=\"%m-%d-%Y\", errors=\"coerce\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return pd.NaT\n",
        "\n",
        "\n",
        "df_total[\"fecha_estandar\"] = df_total[\"fecha\"].apply(estandarizar_fecha)"
      ],
      "metadata": {
        "id": "tdE8or09pOWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total[['fecha','fecha_estandar']]# = pd.to_datetime(df_total['fecha'], errors='coerce')"
      ],
      "metadata": {
        "id": "NorGKnEHbPp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total['fecha_estandar'].isna().sum()"
      ],
      "metadata": {
        "id": "l7oqrmLrqVl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total.drop(columns='fecha', inplace=True)"
      ],
      "metadata": {
        "id": "s5eSb4zMs0c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Seguridad y Cumplimiento (PII)\n",
        "* Protegemos información sensible.\n",
        "\n",
        "* Cumplimos regulación.\n",
        "\n",
        "**Hasheo**\n",
        "Aplicar una función matemática que convierte un dato en una cadena fija de caracteres (un código), de manera determinística.\n",
        "\n",
        "Ejemplo:\n",
        "```\n",
        "usuario@universidad.edu\n",
        "↓\n",
        "3b1f7c4d9e8a...\n",
        "```\n",
        "\n",
        "\n",
        "Tiene longitud fija (ej: 64 caracteres en SHA-256).\n",
        "\n"
      ],
      "metadata": {
        "id": "k9LohyInbcw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def hash_correo(correo):\n",
        "    if pd.isnull(correo):\n",
        "        return None\n",
        "    return hashlib.sha256(correo.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "df_total[\"correo_hash\"] = df_total[\"correo\"].apply(hash_correo)\n",
        "\n",
        "# Eliminamos el correo original\n",
        "df_total = df_total.drop(columns=[\"correo\"])"
      ],
      "metadata": {
        "id": "YjYz2aExnbAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_total.head()"
      ],
      "metadata": {
        "id": "9Jpqk7FsoroN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "te59gQxysx_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Derivación y enriquecimiento"
      ],
      "metadata": {
        "id": "v5hF4Z8ErNJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_total['utilidad'] = df_total['salario'] - df_total['gastos']"
      ],
      "metadata": {
        "id": "K6u3HE8urK5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CARGA (Load)\n",
        "\n",
        "Ahora aplicamos schema-on-write.\n",
        "\n",
        "Primero definimos estructura."
      ],
      "metadata": {
        "id": "KML7vHxKrkPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "\n",
        "conn = duckdb.connect('dw.duckdb')\n",
        "\n",
        "conn.execute(\"\"\"\n",
        "CREATE OR REPLACE TABLE fact_finanzas_etl (\n",
        "    id INTEGER,\n",
        "    salario DOUBLE,\n",
        "    gastos DOUBLE,\n",
        "    fecha DATE,\n",
        "    utilidad DOUBLE,\n",
        "    correo_hash STRING\n",
        ")\n",
        "\"\"\")\n",
        "conn.execute(\"DELETE FROM fact_finanzas_etl\")"
      ],
      "metadata": {
        "id": "A73DR3wQc1tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57617736"
      },
      "source": [
        "\n",
        "\n",
        "print(\"Tablas en la base de datos:\")\n",
        "# Consultar las tablas existentes en la base de datos\n",
        "# Utilizamos 'PRAGMA show_tables;' para DuckDB\n",
        "# Si fuera SQLite, sería 'SELECT name FROM sqlite_master WHERE type='table';'\n",
        "# O en DuckDB también se puede usar 'SHOW TABLES;'\n",
        "tables_df = conn.execute(\"SHOW TABLES;\").fetchdf()\n",
        "display(tables_df)\n",
        "\n",
        "# Si quieres ver el esquema de una tabla específica, por ejemplo, 'fact_finanzas_etl'\n",
        "print(\"Esquema de la tabla fact_finanzas_etl:\")\n",
        "schema_etl_df = conn.execute(\"DESCRIBE fact_finanzas_etl;\").fetchdf()\n",
        "display(schema_etl_df)\n",
        "\n",
        "# Si quieres ver algunas filas de una tabla, por ejemplo, 'fact_finanzas_etl'\n",
        "print(\"Primeras 5 filas de fact_finanzas_etl:\")\n",
        "fact_finanzas_etl_df = conn.execute(\"SELECT * FROM fact_finanzas_etl LIMIT 5;\").fetchdf()\n",
        "display(fact_finanzas_etl_df)\n",
        "\n",
        "# Cerrar la conexión cuando hayas terminado\n",
        "conn.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa98ce21"
      },
      "source": [
        "conn = duckdb.connect('dw.duckdb')\n",
        "\n",
        "conn.register(\"df_transformado\", df_total)\n",
        "\n",
        "conn.execute(\"\"\"\n",
        "INSERT INTO fact_finanzas_etl\n",
        "SELECT id, salario, gastos, fecha_estandar, utilidad, correo_hash\n",
        "FROM df_transformado\n",
        "\"\"\")\n",
        "df_total2=conn.execute(\"SELECT * FROM df_transformado LIMIT 5;\").fetchdf()\n",
        "display(df_total2)\n",
        "conn.unregister(\"df_transformado\")\n",
        "\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f70b47f"
      },
      "source": [
        "## Bloque 4: ELT moderno (Extracción, Carga, Transformación)\n",
        "\n",
        "En ELT, invertimos los pasos: cargamos primero los datos crudos al almacén y luego los transformamos dentro de él usando su motor (por ejemplo, SQL). Los pasos son:\n",
        "\n",
        "ELT Moderno: El Cambio hacia la Flexibilidad y el Cómputo Elástico\n",
        "En el paradigma ELT (Extracción, Carga, Transformación), se invierte el flujo tradicional para adaptarse a la era de la nube. A diferencia del ETL clásico, que depende de un servidor intermedio rígido (Schema-on-Write), el ELT adopta la filosofía de Schema-on-Read, cargando los datos brutos primero para decidir cómo estructurarlos después.\n",
        "Las Fases del Proceso ELT\n",
        "1. Extracción: Se recolectan los datos de fuentes diversas (APIs, bases de datos, logs) en su estado natural y sin procesar. En este paso, no se realiza ninguna limpieza profunda ni cambio de esquema para evitar la pérdida de información original.\n",
        "2. Carga: Los datos se inyectan directamente en el destino final, ya sea un Data Warehouse (como BigQuery o Snowflake) o un Data Lake. Aquí se suelen organizar en una Arquitectura de Medallón, depositándose primero en una capa \"Bronze\" (cruda).\n",
        "3. Transformación: Una vez que los datos residen en el destino, se ejecutan las consultas (usualmente en SQL) para limpiar, normalizar y enriquecer la información.\n",
        "¿Por qué es el estándar actual?\n",
        "*  Aprovechamiento del MPP: Se utiliza la Potencia de Procesamiento Paralelo Masivo de los almacenes de datos modernos, delegando el cómputo pesado a motores escalables en lugar de saturar la memoria de una aplicación cliente.\n",
        "*  Agilidad y Democratización: Al tener los datos crudos disponibles de inmediato, diferentes equipos pueden aplicar sus propias transformaciones según sus necesidades específicas sin depender de procesos previos de TI.\n",
        "*  Red de Seguridad: Mantener una copia fiel de los datos originales permite re-procesar la información meses después si se descubren errores en la lógica de transformación inicial.\n",
        "•* Escalabilidad: Es el enfoque ideal para Big Data y datos no estructurados, permitiendo una ingesta de alta velocidad y baja latencia.\n",
        "\n",
        "\n",
        "### Implementación en Python\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracción y Carga"
      ],
      "metadata": {
        "id": "a8puhJqfvysV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "conn = duckdb.connect(\"dw.duckdb\")\n",
        "\n",
        "# Crear tabla staging cargando CSV directamente\n",
        "conn.execute(\"\"\"\n",
        "CREATE OR REPLACE TABLE staging_raw AS\n",
        "SELECT *\n",
        "FROM read_csv_auto('staging/*.csv')\n",
        "\"\"\")\n",
        "\n",
        "print(\"Datos crudos cargados en staging_raw\")"
      ],
      "metadata": {
        "id": "6bVwfzQzvsFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn.execute(\"\"\"\n",
        "CREATE OR REPLACE TABLE fact_finanzas_elt AS\n",
        "SELECT\n",
        "    id,\n",
        "\n",
        "    -- Validación salario\n",
        "    salario,\n",
        "\n",
        "    -- Imputación gastos\n",
        "    COALESCE(gastos, 0) AS gastos,\n",
        "\n",
        "    -- Estandarización fecha\n",
        "    CASE\n",
        "        WHEN regexp_matches(fecha, '^[0-9]{4}-[0-9]{2}-[0-9]{2}$')\n",
        "            THEN CAST(fecha AS DATE)\n",
        "        WHEN regexp_matches(fecha, '^[0-9]{2}/[0-9]{2}/[0-9]{4}$')\n",
        "            THEN STRPTIME(fecha, '%d/%m/%Y')\n",
        "        WHEN regexp_matches(fecha, '^[0-9]{2}-[0-9]{2}-[0-9]{4}$')\n",
        "            THEN STRPTIME(fecha, '%m-%d-%Y')\n",
        "        ELSE NULL\n",
        "    END AS fecha,\n",
        "\n",
        "    -- Hash SHA256 dentro del motor\n",
        "    CASE\n",
        "        WHEN correo IS NOT NULL\n",
        "            THEN sha256(correo)\n",
        "        ELSE NULL\n",
        "    END AS correo_hash,\n",
        "\n",
        "    -- Regla de negocio\n",
        "    salario - COALESCE(gastos, 0) AS utilidad\n",
        "\n",
        "FROM staging_raw\n",
        "WHERE\n",
        "    id IS NOT NULL\n",
        "    AND fecha IS NOT NULL\n",
        "    AND salario > 0\n",
        "    AND COALESCE(gastos, 0) >= 0\n",
        "\"\"\")\n",
        "\n",
        "print(\"Transformación ELT completada\")"
      ],
      "metadata": {
        "id": "e4ZsYFLXv4BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn.execute(\"\"\"\n",
        "SELECT COUNT(*) FROM fact_finanzas_elt\n",
        "\"\"\").fetchall()"
      ],
      "metadata": {
        "id": "MBo7OmZqwOcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nwek9820wlVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d654385"
      },
      "source": [
        "## Bloque 5: Comparación de tiempos y conclusiones\n",
        "\n",
        "Veamos un ejemplo práctico comparando ambos enfoques. Ejecutamos de nuevo ambos pipelines (ETL y ELT) para medir sus tiempos:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "# ==========================\n",
        "# INICIO MEDICIÓN\n",
        "# ==========================\n",
        "start = time.time()\n",
        "\n",
        "conn = duckdb.connect('dw.duckdb')\n",
        "conn.execute(\"DROP TABLE IF EXISTS fact_finanzas_etl\")\n",
        "\n",
        "dataframes = []\n",
        "\n",
        "# ==========================\n",
        "# 1️ EXTRACCIÓN\n",
        "# ==========================\n",
        "for fname in os.listdir('staging'):\n",
        "    if fname.endswith('.csv'):\n",
        "        df = pd.read_csv(os.path.join('staging', fname))\n",
        "        dataframes.append(df)\n",
        "\n",
        "df_total = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# ==========================\n",
        "#  TRANSFORMACIÓN\n",
        "# ==========================\n",
        "\n",
        "#  Eliminación campos críticos\n",
        "df_total = df_total.dropna(subset=['id', 'fecha'])\n",
        "\n",
        "#  Imputación controlada\n",
        "df_total['gastos'] = df_total['gastos'].fillna(0)\n",
        "\n",
        "#  Validación estructural\n",
        "df_total = df_total[df_total['salario'] > 0]\n",
        "df_total = df_total[df_total['gastos'] >= 0]\n",
        "\n",
        "#  Estandarización fecha\n",
        "def estandarizar_fecha(valor):\n",
        "    if pd.isnull(valor):\n",
        "        return pd.NaT\n",
        "\n",
        "    valor = str(valor).strip()\n",
        "\n",
        "    if \"-\" in valor and len(valor.split(\"-\")[0]) == 4:\n",
        "        return pd.to_datetime(valor, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
        "\n",
        "    if \"/\" in valor:\n",
        "        return pd.to_datetime(valor, format=\"%d/%m/%Y\", errors=\"coerce\")\n",
        "\n",
        "    if \"-\" in valor:\n",
        "        return pd.to_datetime(valor, format=\"%m-%d-%Y\", errors=\"coerce\")\n",
        "\n",
        "    return pd.NaT\n",
        "\n",
        "df_total['fecha_estandar'] = df_total['fecha'].apply(estandarizar_fecha)\n",
        "\n",
        "# Eliminar fechas inválidas\n",
        "df_total = df_total.dropna(subset=['fecha_estandar'])\n",
        "\n",
        "#  Hash SHA256 del correo\n",
        "def hash_correo(correo):\n",
        "    if pd.isnull(correo):\n",
        "        return None\n",
        "    return hashlib.sha256(correo.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "df_total['correo_hash'] = df_total['correo'].apply(hash_correo)\n",
        "\n",
        "# Eliminar columnas originales sensibles\n",
        "df_total = df_total.drop(columns=['correo', 'fecha'])\n",
        "\n",
        "#  Regla de negocio\n",
        "df_total['utilidad'] = df_total['salario'] - df_total['gastos']\n",
        "\n",
        "#  Deduplicación global\n",
        "df_total = df_total.drop_duplicates()\n",
        "\n",
        "# ==========================\n",
        "# 3️ CARGA\n",
        "# ==========================\n",
        "\n",
        "conn.register(\"df_etl\", df_total)\n",
        "\n",
        "conn.execute(\"\"\"\n",
        "CREATE TABLE fact_finanzas_etl AS\n",
        "SELECT\n",
        "    id,\n",
        "    salario,\n",
        "    gastos,\n",
        "    fecha_estandar AS fecha,\n",
        "    utilidad,\n",
        "    correo_hash\n",
        "FROM df_etl\n",
        "\"\"\")\n",
        "\n",
        "conn.unregister(\"df_etl\")\n",
        "conn.close()\n",
        "\n",
        "# ==========================\n",
        "# FIN MEDICIÓN\n",
        "# ==========================\n",
        "etl_time = time.time() - start\n",
        "print(\"Tiempo ETL completo:\", round(etl_time, 3), \"segundos\")"
      ],
      "metadata": {
        "id": "1omQ2Q-SxDHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = time.time()\n",
        "\n",
        "conn = duckdb.connect('dw.duckdb')\n",
        "\n",
        "# Limpieza previa\n",
        "conn.execute(\"DROP TABLE IF EXISTS staging_raw\")\n",
        "conn.execute(\"DROP TABLE IF EXISTS fact_finanzas_elt\")\n",
        "\n",
        "# ==========================\n",
        "# 1 LOAD (Extract + Load)\n",
        "# ==========================\n",
        "\n",
        "conn.execute(\"\"\"\n",
        "CREATE TABLE staging_raw AS\n",
        "SELECT *\n",
        "FROM read_csv_auto('staging/*.csv')\n",
        "\"\"\")\n",
        "\n",
        "# ==========================\n",
        "# TRANSFORM (Dentro del motor)\n",
        "# ==========================\n",
        "\n",
        "conn.execute(\"\"\"\n",
        "CREATE TABLE fact_finanzas_elt AS\n",
        "SELECT DISTINCT\n",
        "    id,\n",
        "\n",
        "    -- Validación salario\n",
        "    salario,\n",
        "\n",
        "    -- Imputación gastos\n",
        "    COALESCE(gastos, 0) AS gastos,\n",
        "\n",
        "    -- Estandarización fecha\n",
        "    CASE\n",
        "        WHEN regexp_matches(fecha, '^[0-9]{4}-[0-9]{2}-[0-9]{2}$')\n",
        "            THEN CAST(fecha AS DATE)\n",
        "        WHEN regexp_matches(fecha, '^[0-9]{2}/[0-9]{2}/[0-9]{4}$')\n",
        "            THEN STRPTIME(fecha, '%d/%m/%Y')\n",
        "        WHEN regexp_matches(fecha, '^[0-9]{2}-[0-9]{2}-[0-9]{4}$')\n",
        "            THEN STRPTIME(fecha, '%m-%d-%Y')\n",
        "        ELSE NULL\n",
        "    END AS fecha,\n",
        "\n",
        "    -- Hash SHA256\n",
        "    CASE\n",
        "        WHEN correo IS NOT NULL\n",
        "            THEN sha256(correo)\n",
        "        ELSE NULL\n",
        "    END AS correo_hash,\n",
        "\n",
        "    -- Regla de negocio\n",
        "    salario - COALESCE(gastos, 0) AS utilidad\n",
        "\n",
        "FROM staging_raw\n",
        "WHERE\n",
        "    id IS NOT NULL\n",
        "    AND fecha IS NOT NULL\n",
        "    AND salario > 0\n",
        "    AND COALESCE(gastos, 0) >= 0\n",
        "\"\"\")\n",
        "\n",
        "elt_time = time.time() - start\n",
        "\n",
        "print(\"Tiempo ELT completo:\", round(elt_time, 3), \"segundos\")"
      ],
      "metadata": {
        "id": "tSHa22flw95c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468771ac"
      },
      "source": [
        "|Categoría|ETL (Tradicional)|ELT (Moderno)|\n",
        "|---|---|---|\n",
        "|Orden de operaciones|Extrae, transforma en un servidor intermedio y luego carga.|Extrae, carga los datos en bruto y luego transforma en el destino.|\n",
        "|Arquitectura|Schema-on-Write: El esquema debe definirse meticulosamente antes de la carga.|Schema-on-Read: Los datos se guardan en bruto; el esquema se aplica al usarlos.|\n",
        "|Ubicación de cómputo|Servidor de procesamiento secundario o motor de ETL externo.|Dentro del almacén de datos (Data Warehouse) o Data Lake.|\n",
        "|Tipos de datos|Ideal para datos estructurados (tablas).|Maneja datos estructurados, semiestructurados y no estructurados (JSON, logs, imágenes).\n",
        "|Escalabilidad|Limitada por el servidor de procesamiento (escalado vertical costoso).|Altamente escalable gracias al procesamiento paralelo masivo (MPP) de la nube."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Rendimiento y Costos\n",
        "* Velocidad: El ELT es generalmente más rápido al cargar grandes volúmenes de datos porque elimina la fase de transformación previa, permitiendo que el sistema de destino procese los datos en paralelo. El ETL puede convertirse en un cuello de botella al procesar registros uno a uno antes de moverlos.\n",
        "* Costos: El ETL requiere una inversión mayor en infraestructura de servidores dedicada y planificación detallada. El ELT aprovecha la elasticidad de la nube (pago por uso) y requiere menos sistemas que mantener, lo que simplifica la pila de datos.\n",
        "#### Seguridad y Cumplimiento\n",
        "* ETL: Es el método preferido para industrias con regulaciones estrictas (como finanzas o salud). Permite limpiar, anonimizar o enmascarar información sensible (PII) antes de que toque el almacenamiento persistente.\n",
        "* ELT: Almacenar datos en bruto requiere un gobierno de datos robusto. Sin embargo, ofrece características de seguridad integradas en los almacenes modernos, como control de acceso granular y auditorías de linaje de datos.\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "#### ¿Cuándo usar cada uno?\n",
        "Elige ETL cuando:\n",
        "* Trabajes con bases de datos heredadas (on-premise) con potencia de procesamiento limitada.\n",
        "* Necesites cumplir con normativas estrictas de privacidad (GDPR, HIPAA) mediante limpieza previa.\n",
        "* El volumen de datos sea pequeño o mediano y altamente predecible.\n",
        "\n",
        "Elige ELT cuando:\n",
        "*  Utilices arquitecturas nativas de la nube como Snowflake, BigQuery o Amazon Redshift.\n",
        "* Manejes Big Data o datos de fuentes cambiantes (sensores de IoT, redes sociales).\n",
        "* Desees mantener una copia fiel de los datos originales (Bronze Layer) para permitir futuros análisis o re-procesamientos sin volver a extraer la información.\n",
        "\n",
        "En la ingeniería de datos contemporánea, la tendencia es hacia un modelo híbrido que prioriza el ELT para la agilidad analítica, mientras utiliza procesos de ETL específicos para la ingesta de datos sensibles o la integración con sistemas antiguos."
      ],
      "metadata": {
        "id": "5sUVgeFQ1eXL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46483fa3"
      },
      "source": [
        "## Bloque 6: Orquestación de flujos de trabajo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5ccdfc"
      },
      "source": [
        "## ¿Qué es la Orquestación de Flujos de Trabajo?\n",
        "\n",
        "Si el ETL/ELT es el proceso de transformar materias primas en productos, la orquestación es el \"director de la orquesta\" que asegura que cada instrumento toque en el momento justo y en armonía. Mientras que la automatización se limita a ejecutar una tarea individual, la orquestación gestiona la secuencia, la lógica de decisión y el intercambio de datos entre sistemas heterogéneos.\n",
        "\n",
        "### Las 4 Funciones Críticas de un Orquestador\n",
        "\n",
        "1.  **Programación (Scheduling):** Automatiza la ejecución basada en intervalos de tiempo (cron), eventos o disparadores externos.\n",
        "2.  **Gestión de Dependencias:** Garantiza que la \"Tarea B\" (ej. transformar) solo inicie si la \"Tarea A\" (ej. extraer) finalizó con éxito. Se visualiza comúnmente como un DAG (Grafo Acíclico Dirigido), que representa el flujo de trabajo sin bucles.\n",
        "3.  **Manejo de Errores y Reintentos:** Implementa lógicas de recuperación automática (retries) ante fallos temporales (como una caída de red) y genera alertas proactivas.\n",
        "4.  **Observabilidad:** Proporciona tableros visuales para monitorear la salud de las tuberías en tiempo real, permitiendo identificar cuellos de botella rápidamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1426446d"
      },
      "source": [
        "### Herramientas Líderes en el Mercado (2025-2026)\n",
        "\n",
        "Dependiendo del entorno y la experiencia del equipo, existen varias opciones:\n",
        "\n",
        "*   **Apache Airflow:** El estándar de la industria. Permite definir flujos mediante código Python puro, lo que ofrece flexibilidad total para tuberías complejas y de gran escala.\n",
        "*   **Mage.ai:** Una alternativa moderna con un enfoque híbrido. Combina la interactividad de los notebooks con la robustez de una herramienta de producción, facilitando la visualización de datos en cada bloque de código.\n",
        "*   **Prefect:** Diseñado para la simplicidad, utiliza decoradores de Python para convertir funciones estándar en tareas orquestadas, ideal para flujos dinámicos.\n",
        "*   **Dagster:** Se centra en los activos de datos (tablas resultantes) en lugar de solo en las tareas, priorizando el linaje y la calidad del dato.\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "### Idempotencia\n",
        "\n",
        "En un entorno profesional, es imperativo que las tareas sean **idempotentes**. Esto significa que si una tarea se ejecuta varias veces con la misma entrada, el resultado final debe ser siempre el mismo.\n",
        "\n",
        "*   **Por qué es vital:** Si una carga de datos falla a la mitad y el orquestador la reintenta, la idempotencia evita que se dupliquen registros o se corrompa la base de datos.\n",
        "*   **Patrón común:** Usar la lógica de \"Eliminar e Insertar\" (Delete + Insert) para el periodo específico de tiempo que se está procesando."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-airflow --quiet\n"
      ],
      "metadata": {
        "id": "2eKRscG04SjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Airflow\n",
        "\n",
        "1. ¿Qué es Apache Airflow?\n",
        "Apache Airflow es la plataforma estándar de la industria para orquestar flujos de trabajo de datos. Su filosofía es \"Workflows as Code\" (flujos como código), lo que permite definir procesos mediante Python, facilitando el control de versiones, pruebas y colaboración.\n",
        "* DAG (Directed Acyclic Graph): Es el plano de tu flujo. Es una colección de tareas organizadas para definir su orden de ejecución y dependencias.\n",
        "* Operadores: Plantillas que definen qué hace cada tarea (ej. ejecutar un script de Python, una consulta SQL o un comando de terminal).\n",
        "* Tareas: La instancia real de un operador dentro de un DAG.\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "2. Tutorial: Creando tu primer Pipeline (TaskFlow API)\n",
        "La forma más moderna y \"Pythonica\" de escribir DAGs es mediante la TaskFlow API (introducida en la versión 2.0), que utiliza decoradores para simplificar el código.\n",
        "\n",
        "La orquestación integra múltiples tareas automatizadas para completar un proceso de extremo a extremo, asegurando que los datos se compartan efectivamente y se activen en el orden correcto.\n",
        "\n",
        "* @dag (Decorador de Grafo Acíclico Dirigido):\n",
        "    >* Qué hace: Define el contenedor principal del flujo de trabajo. Establece la frecuencia de ejecución (schedule), la fecha de inicio y las propiedades globales del pipeline.\n",
        "    >* Cómo funciona: Airflow lee este decorador para registrar el proceso en su planificador (Scheduler), permitiendo que el flujo sea visible y programable en la interfaz web.\n",
        "\n",
        "* @task (Decorador de Tarea):\n",
        "    >* Qué hace: Convierte una función de Python estándar en una unidad de trabajo atómica.\n",
        "    >* Cómo funciona: Cada función marcada con @task se convierte en un nodo dentro del DAG. Airflow gestiona los reintentos (retries) y el aislamiento de esta tarea; si falla, se puede reintentar sin reiniciar todo el pipeline.\n",
        "\n",
        "* Sensor (Operador Especializado):\n",
        "    >* Qué hace: Es un tipo de operador que espera una condición externa antes de permitir que el flujo continúe.\n",
        "    >* Cómo funciona: Por ejemplo, un sensor puede monitorear la llegada de un archivo a un bucket de S3 o la finalización de un proceso en una base de datos externa antes de activar la siguiente tarea.\n",
        "\n",
        "* .expand() y .partial() (Mapeo Dinámico):\n",
        "    >* Qué hace: Permite que una tarea se ejecute múltiples veces en paralelo, una por cada entrada de una lista.\n",
        "    >* Cómo funciona: partial() define los parámetros que permanecen constantes (como credenciales), mientras que expand() crea n copias de la tarea para procesar archivos o modelos de forma concurrente."
      ],
      "metadata": {
        "id": "bg4MV-P_URM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Arquitectura y Ejecución\n",
        "Para ejecutar Airflow, el sistema se apoya en componentes clave que interactúan de forma asíncrona:\n",
        "> 1. Scheduler (Planificador): El cerebro que monitorea los DAGs y activa las tareas cuando sus dependencias se cumplen.\n",
        ">2. Webserver: La interfaz de usuario para monitorear estados, ver logs y activar tareas manualmente.\n",
        ">3. Metadata Database: Donde se guarda el historial de ejecuciones y estados (usualmente PostgreSQL o MySQL).\n"
      ],
      "metadata": {
        "id": "heX59sk2UpcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from airflow.decorators import dag, task # Revertido a airflow.decorators\n",
        "import duckdb\n",
        "import time\n",
        "\n",
        "\n",
        "default_args = {\n",
        "    \"owner\": \"data_engineer\",\n",
        "    \"retries\": 1,\n",
        "    \"retry_delay\": timedelta(minutes=2),\n",
        "}\n",
        "\n",
        "\n",
        "@dag(\n",
        "    dag_id=\"elt_duckdb_pipeline\",\n",
        "    default_args=default_args,\n",
        "    schedule=\"@daily\", # Cambiado de schedule_interval a schedule\n",
        "    start_date=datetime(2025, 1, 1),\n",
        "    catchup=False,\n",
        "    description=\"Pipeline ELT con DuckDB orquestado por Airflow\",\n",
        ")\n",
        "def elt_pipeline():\n",
        "\n",
        "    @task()\n",
        "    def limpiar_tablas():\n",
        "        conn = duckdb.connect(\"dw.duckdb\")\n",
        "        conn.execute(\"DROP TABLE IF EXISTS staging_raw\")\n",
        "        conn.execute(\"DROP TABLE IF EXISTS fact_finanzas_elt2\")\n",
        "        conn.close()\n",
        "        return \"Tablas limpiadas\"\n",
        "\n",
        "    @task()\n",
        "    def cargar_staging(mensaje):\n",
        "        conn = duckdb.connect(\"dw.duckdb\")\n",
        "        conn.execute(\"\"\"\n",
        "            CREATE TABLE staging_raw AS\n",
        "            SELECT *\n",
        "            FROM read_csv_auto('staging/*.csv')\n",
        "        \"\"\")\n",
        "        conn.close()\n",
        "        return \"Staging cargado\"\n",
        "\n",
        "    @task()\n",
        "    def transformar_datos(mensaje):\n",
        "        conn = duckdb.connect(\"dw.duckdb\")\n",
        "        conn.execute(\"\"\"\n",
        "            CREATE TABLE fact_finanzas_elt AS\n",
        "            SELECT DISTINCT\n",
        "                id,\n",
        "                salario,\n",
        "                COALESCE(gastos, 0) AS gastos,\n",
        "                CASE\n",
        "                    WHEN regexp_matches(fecha, '^[0-9]{4}-[0-9]{2}-[0-9]{2}$')\n",
        "                        THEN CAST(fecha AS DATE)\n",
        "                    WHEN regexp_matches(fecha, '^[0-9]{2}/[0-9]{2}/[0-9]{4}$')\n",
        "                        THEN STRPTIME(fecha, '%d/%m/%Y')\n",
        "                    WHEN regexp_matches(fecha, '^[0-9]{2}-[0-9]{2}-[0-9]{4}$')\n",
        "                        THEN STRPTIME(fecha, '%m-%d-%Y')\n",
        "                    ELSE NULL\n",
        "                END AS fecha,\n",
        "                CASE\n",
        "                    WHEN correo IS NOT NULL\n",
        "                        THEN sha256(correo)\n",
        "                    ELSE NULL\n",
        "                END AS correo_hash,\n",
        "                salario - COALESCE(gastos, 0) AS utilidad\n",
        "            FROM staging_raw\n",
        "            WHERE\n",
        "                id IS NOT NULL\n",
        "                AND fecha IS NOT NULL\n",
        "                AND salario > 0\n",
        "                AND COALESCE(gastos, 0) >= 0\n",
        "        \"\"\")\n",
        "        conn.close()\n",
        "        return \"Transformación completada\"\n",
        "\n",
        "    @task()\n",
        "    def medir_tiempo():\n",
        "        # Esta tarea solo muestra que podemos agregar lógica adicional\n",
        "        return f\"Ejecución completada en {datetime.now()}\"\n",
        "\n",
        "    limpieza = limpiar_tablas()\n",
        "    staging = cargar_staging(limpieza)\n",
        "    transformacion = transformar_datos(staging)\n",
        "    medir_tiempo()\n",
        "\n",
        "\n",
        "elt_dag = elt_pipeline()"
      ],
      "metadata": {
        "id": "orL6EA9s4KOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53a40e75"
      },
      "source": [
        "print(elt_dag)\n",
        "print(type(elt_dag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d4eb6ad"
      },
      "source": [
        "### Referencias\n",
        "\n",
        "Rubin, D. B. (1976). Inference and missing data. *Biometrika*, *63*(3), 581–590.\n",
        "\n",
        "Software Foundation. (s.f.). *Architecture Overview — Airflow 3.1.7 Documentation*. Apachecuperado de [https://airflow.apache.org/]."
      ]
    }
  ]
}