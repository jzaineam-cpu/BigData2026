{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazaineam1/BigData2026/blob/main/Cuadernos/3_Arquitectura%20Big%20Data%20End-to-End.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Arquitectura Empresarial y Ciclo de vida de los Datos***\n",
        "\n",
        "## ***Universidad Central***\n",
        ">## **Facultad de Ingeniería y Ciencias Básicas.**\n",
        ">## ***Maestría en analítica de datos***\n",
        "![Imágen1](https://www.ucentral.edu.co/sites/default/files/logo_1.png)\n",
        "\n",
        "\n",
        ">## ***Big Data.***\n",
        ">## ***Docente: Antonino Zainea Maya.***"
      ],
      "metadata": {
        "id": "-JlEuwKXldMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de comenzar ...\n",
        "\n",
        "¿Dónde nacen los datos dentro de una organización?\n",
        "\n",
        "¿Todos los datos son estructurados?\n",
        "\n",
        "¿Es lo mismo almacenar que analizar?\n",
        "\n",
        "¿Puede una empresa hacer análisis directamente sobre su sistema transaccional?\n",
        "\n",
        "¿Qué problemas surgirían si no existiera separación entre sistemas operativos y analíticos?"
      ],
      "metadata": {
        "id": "o2DvdwCbmQI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio Inicial — Simulación del Nacimiento del Dato\n",
        "> 1. Crea un archivo .xlsx\n",
        ">2. Crear las siguientes columnas:\n",
        "\n",
        "|ID | Nombre | Edad | Salario | Gastos | año|\n",
        "|---|---|---|---|---|---|\n",
        "\n",
        ">3. Agregar un registro (datos ficticios), como sigue.\n",
        "\n",
        "|ID | Nombre | Edad | Salario | Gastos | año|\n",
        "|---|---|---|---|---|---|\n",
        "|111 | Ana | 28 | 4000000 | 2500000 | 2024|\n",
        "\n",
        "> 4. Guardar el archivo como `ID_año.xlsx`, en nuestro ejemplo:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "111_2024.xlsx\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "> 5. Crear una imagen simple escribir únicamente un número grande (ejemplo: 1500000). Ese número representará una “ganancia adicional”.\n",
        "> 6. Guardar la imagen como `ID_año.png` o con el formato adecuado, en nuestro ejemplo:\n",
        "\n",
        "```\n",
        "111_2024.png\n",
        "```\n",
        "\n",
        "> 7. Carga la información en [https://drive.google.com/drive/folders/1F7t1Yyep4sEl9D9QjDzVWTwP6r4o1LiL?usp=sharing](https://drive.google.com/drive/folders/1F7t1Yyep4sEl9D9QjDzVWTwP6r4o1LiL?usp=sharing)\n",
        "\n",
        "![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/1.png)"
      ],
      "metadata": {
        "id": "z93aEpkDmdOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ol92sx_r6Dq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d060abc"
      },
      "source": [
        "## ¿Por qué comenzamos con un archivo `XLSX`?\n",
        "\n",
        "El ejercicio de crear un archivo `.xlsx` con registros individuales como `ID`, `Nombre`, `Edad`, `Salario`, `Gastos` y `Año`, y luego guardar una imagen con una \"ganancia adicional\", **simula de forma simplificada el nacimiento de los datos en un sistema transaccional.**\n",
        "<img src=\"\" alt=\"\" width=\"50\"/>\n",
        "En una empresa real, estos datos no nacerían en un archivo de Excel, sino en un **Sistema de Procesamiento de Transacciones Online (OLTP por sus siglas en inglés: Online Transaction Processing)**.\n",
        "\n",
        "### **¿Qué es un Sistema OLTP (Online Transaction Processing)?**\n",
        "\n",
        "Un sistema OLTP está diseñado para capturar y procesar transacciones de datos en tiempo real, donde cada interacción, cada venta, cada registro de cliente, cada movimiento de inventario se registra instantáneamente.\n",
        "\n",
        "**Características clave de un sistema OLTP:**\n",
        "\n",
        "*   **Orientado a Transacciones:** Su función principal es registrar y gestionar operaciones individuales de forma eficiente.\n",
        "*   **Rapidez en Escritura (INSERT, UPDATE, DELETE):** Están optimizados para ejecutar rápidamente pequeñas transacciones que insertan, actualizan o eliminan registros.\n",
        "*   **Consistencia y Atomicidad:** Aseguran que cada transacción se complete por completo o no se realice en absoluto (propiedad ACID: Atomicidad, Consistencia, Aislamiento, Durabilidad).\n",
        "*   **Alta Concurrencia:** Permiten que miles o millones de usuarios realicen transacciones simultáneamente sin conflictos.\n",
        "*   **Datos Actuales y Detallados:** Contienen los datos más recientes y a nivel de detalle más bajo, representando el estado actual del negocio.\n",
        "*   **Estructura Rígida y Normalizada:** Suelen tener esquemas de base de datos muy normalizados para reducir la redundancia y asegurar la integridad de los datos.\n",
        "\n",
        "Todas estas operaciones se realizan en milisegundos, garantizando la consistencia de los datos en tiempo real. **El archivo XLSX del ejercicio, con su registro individual y la imagen de \"ganancia adicional\", simula esta naturaleza de datos atómicos y actuales que se generan constantemente en un OLTP.**\n",
        "\n",
        "\n",
        "En una empresa real, ese archivo no existiría como xlsx.\n",
        "Existiría como:\n",
        "\n",
        "- Base de datos\n",
        "- CRM (Customer Relationship Management)\n",
        "\n",
        "### **El Problema Empresarial: ¿Por qué no analizar directamente en un OLTP?**\n",
        "\n",
        "Aunque los sistemas OLTP son excelentes para la operación diaria, **NO están optimizados para el análisis de datos complejos o históricos.** Aquí radica el \"problema empresarial\":\n",
        "\n",
        "1.  **Impacto en el Rendimiento Operacional:**\n",
        "2.  **No Optimizado para Agregaciones y Tendencias:**\n",
        "3.  **Datos Actuales vs. Históricos:**\n",
        "\n",
        "## Centralización → Data Lake (donde guardaríamos xlsx + imagen)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9197b6d"
      },
      "source": [
        "\n",
        "Un **Data Lake** es un repositorio centralizado y escalable, fundamental en arquitecturas de Big Data, donde se almacenan grandes volúmenes de datos en su **forma original y sin procesar**. Esto es clave: no exige una transformación inmediata ni la imposición de un esquema rígido en el momento de la ingesta (conocido como _schema-on-read_).\n",
        "\n",
        "Esto implica que un Data Lake puede:\n",
        "\n",
        "*   **Almacenar datos de cualquier tipo:** Desde archivos estructurados (como CSV, XLSX, datos de bases de datos relacionales), semiestructurados (JSON, XML), hasta no estructurados (imágenes, audios, videos, PDFs, logs, correos electrónicos). La flexibilidad es su gran fortaleza.\n",
        "*   **Retener datos en su estado crudo:** No es necesario limpiar, transformar o definir la estructura de los datos antes de guardarlos. Esto reduce el tiempo y el esfuerzo inicial, y permite que los datos se utilicen para diversos propósitos futuros sin limitaciones por una estructura predefinida.\n",
        "*   **Escalar de forma masiva y rentable:** Están diseñados para manejar volúmenes de datos que van desde terabytes hasta petabytes, con costos de almacenamiento generalmente más bajos que los sistemas tradicionales de bases de datos.\n",
        "\n",
        "Es fundamental comprender las **funciones y limitaciones de un Data Lake en el contexto de Big Data**:\n",
        "\n",
        "*   El Data Lake **no analiza** los datos por sí mismo. Su propósito no es realizar consultas complejas o generar informes directos.\n",
        "*   El Data Lake **no transforma** activamente los datos. Si bien puede albergar herramientas para transformación (ETL/ELT), no es su función intrínseca modificarlos al almacenar.\n",
        "*   Su rol principal es el **almacenamiento flexible, masivo y la centralización** de datos brutos.\n",
        "\n",
        "### **¿Cómo se refleja el Data Lake en nuestro ejercicio?**\n",
        "\n",
        "En el ejercicio práctico, ustedes crearon:\n",
        "\n",
        "*   Un archivo `.xlsx` (representando un dato estructurado originado en un OLTP).\n",
        "*   Una imagen `.png` con un número (representando un dato no estructurado , como una \"ganancia adicional\").\n",
        "\n",
        "Si ahora subimos esos archivos a un servicio de almacenamiento en la nube, como un **bucket de Cloud Storage**, **ese bucket se convierte en nuestro Data Lake de simulación.**\n",
        "\n",
        "**¿Por qué este bucket actúa como nuestro Data Lake?**\n",
        "\n",
        "*   **Centralización:** Está reuniendo y almacenando archivos de múltiples fuentes (en este caso, de múltiples estudiantes/simulaciones de OLTP).\n",
        "*   **Almacenamiento de Datos en su Forma Original:** Guarda el `.xlsx` tal como lo crearon y la imagen `.png` sin procesarla ni modificarla.\n",
        "*   **Sin Transformación ni Validación Inmediata:** No está transformando los datos ni validando reglas de negocio al momento de la ingesta. Simplemente los guarda.\n",
        "*   **Escalabilidad Implícita:** Los servicios de almacenamiento en la nube están inherentemente diseñados para escalar y almacenar grandes volúmenes de datos.\n",
        "\n",
        "En este punto del flujo de datos, el Data Lake **no se preocupa si el `.xlsx` tiene errores, si los datos son consistentes, o si la imagen es legible en términos de negocio**. Su función es puramente de **conservación** de los datos crudos.\n",
        "\n",
        "**En términos del Ciclo de Vida de los Datos:**\n",
        "\n",
        "1.  **Nacimiento (Generación):** Ocurre en el OLTP (simulado por la creación del archivo `.xlsx` por el estudiante y la imagen).\n",
        "2.  **Centralización (Ingesta):** Los datos se mueven al Data Lake (simulado por la subida de los archivos al bucket de Cloud Storage/Drive).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuestro Datalake (Cloud storage)\n",
        "\n",
        "<img src=\"https://blog.nashtechglobal.com/wp-content/uploads/2023/09/Google-Cloud-Storage-Reviews-1024x512-20200419.png\" alt=\"Excel Icon\" width=\"300\"/>\n",
        "\n",
        "[Documentación](https://cloud.google.com/learn/what-is-cloud-storage?hl=es)\n",
        "\n",
        "[**Buckets**](https://docs.cloud.google.com/storage/docs/buckets?hl=es-419)\n",
        "\n",
        "Los buckets son los contenedores básicos que conservan tus datos como objetos. Todo lo que almacenes en Cloud Storage debe estar contenido en un bucket. Puedes usar buckets para organizar tus datos y controlar el acceso a ellos, pero, a diferencia de los directorios y las carpetas, no puedes anidar los buckets."
      ],
      "metadata": {
        "id": "UJ4iIvbs3amn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como crear un bucket\n",
        "1. ingresa a gcp concole.google.com\n",
        "2. busca cloud storage\n",
        "3. da click en crea bucket\n",
        "4. selecciona tu opcion preferida para el bucket (da una breve explicacion)\n",
        "5. al final tendras algo como esto ![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/4.png)"
      ],
      "metadata": {
        "id": "thjakIQV4lej"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ac541dd"
      },
      "source": [
        "### **Cómo crear un Bucket en Google Cloud Storage**\n",
        "\n",
        "Para crear un contenedor de datos (bucket) en Google Cloud Storage, sigue estos pasos:\n",
        "\n",
        "1.  **Accede a la Consola de GCP:** Ingresa a [console.cloud.google.com](https://console.cloud.google.com/) con tu cuenta de Google Cloud.\n",
        "2.  **Navega a Cloud Storage:** En la barra de búsqueda superior, escribe \"Cloud Storage\" y selecciona el servicio correspondiente.\n",
        "3.  **Crea un Bucket Nuevo:** Haz clic en el botón \"Crear bucket\".\n",
        "4.  **Configura los Detalles del Bucket:** Se te presentarán varias opciones importantes:\n",
        "    *   **Nombre del bucket:** Debe ser globalmente único (como un dominio web), sin espacios y en minúsculas (ej: `mi-datalake-uni-central-2024`).\n",
        "    *   **Tipo de ubicación:**\n",
        "        *   **Región:** Para baja latencia en una ubicación geográfica específica (ej: `southamerica-east1` para São Paulo).\n",
        "        *   **Multirregión:** Para alta disponibilidad y redundancia geográfica (ej: `US` para Estados Unidos o `EUROPE` para Europa).\n",
        "        *   **Doble región:** Combina la baja latencia de dos regiones específicas con alta disponibilidad entre ellas.\n",
        "    *   **Clase de almacenamiento:** Define el costo y la disponibilidad de los datos:\n",
        "        *   **Standard:** Acceso frecuente (ideal para datos en uso activo).\n",
        "        *   **Nearline:** Acceso mensual (para copias de seguridad o archivos que se consultan esporádicamente).\n",
        "        *   **Coldline:** Acceso trimestral (para archivado o datos que se consultan muy rara vez).\n",
        "        *   **Archive:** Acceso anual (para archivado a largo plazo con el menor costo).\n",
        "    *   **Control de acceso:** Decide cómo se gestionarán los permisos.\n",
        "        *   **Uniforme:** Simplifica la gestión de accesos al usar solo IAM (Identity and Access Management) para todo el bucket.\n",
        "        *   **Fino:** Permite el control de accesos tanto con IAM como con Listas de Control de Acceso (ACLs) para objetos individuales (más complejo, pero granular).\n",
        "5.  **Revisa y Confirma:** Al final, tendrás un bucket configurado, listo para almacenar tus objetos. Visualmente, se verá similar a esto:\n",
        "\n",
        "    ![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/4.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yBA8uvkjzMmc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdfa9969"
      },
      "source": [
        "## ¿Qué es Google Apps Script?\n",
        "\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Google_Apps_Script.svg\" alt=\"\" width=\"200\"/>\n",
        "\n",
        "**Google Apps Script (GAS)** es una plataforma de desarrollo basada en la nube que permite extender y automatizar funcionalidades en los productos de Google Workspace (anteriormente G Suite) y más allá. Utiliza JavaScript como lenguaje de programación, lo que lo hace accesible para desarrolladores web y no tan especializados. Se accede y gestiona a través de la URL [script.google.com](https://script.google.com).\n",
        "\n",
        "### Propósito Principal\n",
        "\n",
        "El propósito fundamental de Google Apps Script es **conectar, extender y automatizar** el ecosistema de Google Workspace, facilitando la creación de soluciones personalizadas que mejoren la productividad y eficiencias empresariales. Permite a los usuarios y desarrolladores construir aplicaciones ligeras que interactúan con los datos y servicios de Google de manera programática, sin la necesidad de desplegar o mantener servidores.\n",
        "\n",
        "### Entorno de Ejecución\n",
        "\n",
        "Google Apps Script se ejecuta completamente en la **nube de Google**. Esto significa que los scripts no requieren ninguna infraestructura local por parte del usuario. Los scripts se almacenan en los servidores de Google y se ejecutan desde allí, lo que ofrece las siguientes ventajas:\n",
        "\n",
        "*   **Sin instalación:** No es necesario instalar ningún software o SDK en tu máquina.\n",
        "*   **Accesibilidad:** Puedes acceder y editar tus scripts desde cualquier navegador web.\n",
        "*   **Escalabilidad y Mantenimiento:** Google se encarga de la infraestructura, el escalado y el mantenimiento de los servidores donde se ejecutan tus scripts.\n",
        "\n",
        "### Principales Capacidades\n",
        "\n",
        "Las capacidades de Google Apps Script son amplias y versátiles:\n",
        "\n",
        "1.  **Automatización de Tareas en Google Workspace:**\n",
        "    *   **Hojas de Cálculo (Sheets):** Automatizar informes, manipular datos, crear menús personalizados, enviar alertas basadas en contenido de celdas.\n",
        "    *   **Documentos (Docs):** Generar documentos automáticamente, realizar fusiones de correspondencia, aplicar formato condicional.\n",
        "    *   **Drive:** Organizar archivos, gestionar permisos, crear flujos de trabajo de aprobación.\n",
        "    *   **Gmail:** Enviar correos electrónicos personalizados masivos, filtrar mensajes, crear respuestas automáticas, generar borradores.\n",
        "    *   **Calendario (Calendar):** Programar eventos, sincronizar calendarios, enviar recordatorios.\n",
        "    *   **Formularios (Forms):** Procesar respuestas automáticamente, validar entradas, enviar notificaciones.\n",
        "    *   **Presentaciones (Slides):** Crear presentaciones dinámicas a partir de datos, actualizar contenido.\n",
        "\n",
        "2.  **Integración con Servicios de Google:**\n",
        "    *   Permite interactuar con otras APIs de Google como Google Translate, Google Maps, YouTube, Google Cloud Vision, etc., facilitando la creación de soluciones híbridas y potentes.\n",
        "\n",
        "[Documentación](https://developers.google.com/apps-script/overview?hl=es-419)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si deseas realizar este ejercicio, debes ingresar a [https://script.google.com/](https://script.google.com/)\n",
        "\n",
        "1. Crear un nuevo proyecto llamado `Sync Drive to Cloud Storage`\n",
        "![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/2.png)"
      ],
      "metadata": {
        "id": "6VAzefLP1sMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. en configuración activa\n",
        "Mostrar el archivo de manifiesto \"appsscript.json\" en el editor\n",
        "![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/3.png)\n",
        "3. En el editor incluye esta configuración  a appscript.json\n",
        "\n",
        "```\n",
        "{\n",
        "  \"timeZone\": \"America/Bogota\",\n",
        "  \"dependencies\": {\n",
        "  },\n",
        "  \"exceptionLogging\": \"STACKDRIVER\",\n",
        "  \"runtimeVersion\": \"V8\",\n",
        "  \"oauthScopes\": [\n",
        "    \"https://www.googleapis.com/auth/drive.readonly\",\n",
        "    \"https://www.googleapis.com/auth/devstorage.read_write\",\n",
        "    \"https://www.googleapis.com/auth/script.external_request\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "y a codigo.gs\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "/**\n",
        " * ID de la carpeta de Google Drive donde están los archivos de estudiantes\n",
        " */\n",
        "const DRIVE_FOLDER_ID = \"1F7t1Yyep4sEl9D9QjDzVWTwP6r4o1LiL\";\n",
        "\n",
        "/**\n",
        " * Tu nombre de bucket en Cloud Storage\n",
        " */\n",
        "const STORAGE_BUCKET = \"datalake-estudiantes\";\n",
        "\n",
        "/**\n",
        " * Función principal que revisa la carpeta de Drive y copia archivos nuevos a Cloud Storage\n",
        " */\n",
        "function syncDriveFolderToGCS() {\n",
        "  // Obtener la carpeta de Drive\n",
        "  const folder = DriveApp.getFolderById(DRIVE_FOLDER_ID);\n",
        "  \n",
        "  // Lista todos los archivos dentro de esa carpeta\n",
        "  const files = folder.getFiles();\n",
        "  \n",
        "  while (files.hasNext()) {\n",
        "    const file = files.next();\n",
        "    \n",
        "    // Obtener el blob del archivo\n",
        "    const blob = file.getBlob();\n",
        "    \n",
        "    // Crear URL de la API de Cloud Storage\n",
        "    const url = `https://storage.googleapis.com/upload/storage/v1/b/${STORAGE_BUCKET}/o?uploadType=media&name=${encodeURIComponent(\"raw/\" + file.getName())}`;\n",
        "\n",
        "    // Hacer la subida a GCS\n",
        "    const response = UrlFetchApp.fetch(url, {\n",
        "      method: \"POST\",\n",
        "      contentType: blob.getContentType(),\n",
        "      payload: blob.getBytes(),\n",
        "      headers: {\n",
        "        Authorization: \"Bearer \" + ScriptApp.getOAuthToken()\n",
        "      }\n",
        "    });\n",
        "    \n",
        "    console.log(\"Archivo subido:\", file.getName());\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yDysvunI2rQN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9f8c323"
      },
      "source": [
        "## Cloud Run — ¿Qué papel cumple?\n",
        "\n",
        "Una vez que Eventarc detecta la llegada de un nuevo archivo a nuestro Data Lake, su siguiente paso es **activar** un servicio que se encargará del procesamiento. En nuestro caso, ese servicio es **Cloud Run**.\n",
        "\n",
        "### **¿Qué es Cloud Run?**\n",
        "\n",
        "**Cloud Run** es una plataforma de cómputo **serverless** que te permite ejecutar contenedores sin estado a través de solicitudes web o eventos. Es la capa de procesamiento ideal para nuestra arquitectura event-driven.\n",
        "\n",
        "Aquí, en Cloud Run, es donde ocurre algo clave en el ciclo de vida de los datos:\n",
        "\n",
        "### **Transformación de Datos**\n",
        "\n",
        "Hasta ahora, nuestro flujo ha sido:\n",
        "\n",
        "1.  **Nacimiento (Generación):** Ocurre en el OLTP (simulado por la creación del archivo `.xlsx` y la imagen).\n",
        "2.  **Centralización (Ingesta):** Los datos se mueven al Data Lake (simulado por la subida de los archivos al bucket de Cloud Storage/Drive).\n",
        "3.  Ahora entramos en → **Procesamiento y Transformación**\n",
        "\n",
        "Cuando Cloud Run es activado por Eventarc tras la llegada de un nuevo archivo al Data Lake, su lógica se encargará de clasificar y procesar esos datos:\n",
        "\n",
        "*   **Si es un archivo Excel (`.xlsx`):** Cloud Run ejecutará un proceso diseñado para extraer, limpiar y transformar los datos estructurados que contiene (ej. validar tipos de datos, calcular nuevos campos, estandarizar formatos). Los datos procesados se almacenarían en otra capa, como un Data Warehouse o un Data Mart.\n",
        "*   **Si es una imagen (`.png`):** Cloud Run activará un proceso para extraer información de la imagen (ej. reconocimiento óptico de caracteres para leer el número, análisis de contenido para metadatos). Los resultados de este procesamiento se almacenarían de forma adecuada, quizás en una base de datos NoSQL o de nuevo en el Data Lake pero en una zona \"procesada\".\n",
        "\n",
        "Esto introduce el concepto fundamental de **separación por tipo de dato** o **procesamiento diferenciado**:\n",
        "\n",
        "*   **Pipelines diferentes para diferentes formatos:** Es común en Big Data tener flujos de procesamiento (pipelines) especializados para datos estructurados, semi-estructurados y no estructurados.\n",
        "*   **Tratamientos distintos según la naturaleza del dato:** Los requisitos de transformación y los algoritmos aplicados varían enormemente entre un archivo de ventas en Excel y una imagen de ganancia. Cloud Run es lo suficientemente flexible para manejar esta diversidad.\n",
        "\n",
        "### **Ventajas de Cloud Run en una Arquitectura Empresarial:**\n",
        "\n",
        "Cloud Run representa la **computación serverless**, lo que significa:\n",
        "\n",
        "*   **No mantiene servidores activos:** Pagas solo por el tiempo que tu código se está ejecutando. No hay servidores que provisionar o gestionar.\n",
        "*   **Se ejecuta bajo demanda:** Se activa solo cuando un evento (como el de Eventarc) lo dispara, o cuando recibe una solicitud web.\n",
        "*   **Escala automáticamente:** Puede escalar desde cero instancias hasta miles en segundos, manejando picos de demanda sin configuración manual.\n",
        "\n",
        "En una empresa grande o en otras plataformas cloud, este rol de \"procesador bajo demanda\" podría ser desempeñado por:\n",
        "\n",
        "*   **Lambda (AWS)**\n",
        "*   **Azure Functions (Microsoft Azure)**\n",
        "*   **Kubernetes (si se ejecuta en un clúster, con un enfoque más de contenedores)**\n",
        "*   **Spark jobs (para procesamiento masivo de lotes, a menudo orquestado por otros servicios)**\n",
        "\n",
        "Pero el rol es el mismo: **ejecutar el código de transformación y procesamiento de datos** que convierte los datos crudos del Data Lake en información valiosa, lista para el análisis.\n",
        "\n",
        "### **¿Qué tiene que ver Cloud Run con Big Data?**\n",
        "\n",
        "En el mundo de Big Data, nos enfrentamos a:\n",
        "\n",
        "*   **Volumen variable de datos:** La cantidad de datos que llegan puede cambiar drásticamente de un momento a otro.\n",
        "*   **Carga impredecible:** Los picos de demanda de procesamiento no son constantes y pueden ocurrir en cualquier momento.\n",
        "*   **Necesidad de procesamiento distribuido:** Para manejar grandes volúmenes eficientemente, el trabajo debe dividirse y ejecutarse en paralelo.\n",
        "\n",
        "Si mañana, en lugar de 10 archivos, llegan **10.000 archivos al mismo tiempo**:\n",
        "\n",
        "*   Un servidor tradicional podría **colapsar** o ralentizarse significativamente.\n",
        "*   Un proceso manual **no escalaría** y se generaría un enorme cuello de botella.\n",
        "\n",
        "**Cloud Run representa la Elasticidad en Big Data.** Sus características clave son:\n",
        "\n",
        "*   **Procesamiento Elástico:** Se adapta automáticamente a la carga de trabajo. Si hay muchos eventos (muchos archivos llegando), Cloud Run instantáneamente \"se multiplica\" creando más instancias para procesarlos en paralelo. Cuando la carga disminuye, las instancias se reducen a cero.\n",
        "*   **Se ejecuta solo cuando llega un evento:** Pago por uso, optimizando costos al no mantener recursos inactivos.\n",
        "*   **Se multiplica si la carga aumenta:** Proporciona la capacidad de procesamiento necesaria para manejar picos inesperados, garantizando la **disponibilidad** y **velocidad** del procesamiento incluso bajo demanda extrema.\n",
        "*   **Desaparece (se escala a cero):** Cuando no hay trabajo, no hay costos. Esto es la **elasticidad** llevada al máximo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c98d58"
      },
      "source": [
        "## Eventarc — ¿Por qué aparece ahora?\n",
        "\n",
        "Hasta ahora, nuestro Data Lake (simulado por el bucket de Cloud Storage/Drive) solo cumplía la función de **almacenar** los datos en su forma original. Pero en una arquitectura empresarial real, especialmente en el contexto de Big Data, simplemente almacenar no es suficiente. Necesitamos **reaccionar** de manera automática cuando algo nuevo ocurre, como la llegada de nuevos datos.\n",
        "\n",
        "Pensemos en el escenario de nuestro ejercicio: si 200 estudiantes suben archivos, no es viable ni eficiente que alguien revise manualmente el bucket de Cloud Storage constantemente para ver si hay algo nuevo que procesar.\n",
        "\n",
        "Aquí es donde entra en juego el concepto de **arquitectura orientada a eventos** y, en Google Cloud, el servicio que facilita esto es **Eventarc**.\n",
        "\n",
        "### **¿Qué es Eventarc?**\n",
        "\n",
        "**Eventarc** es una plataforma de eventos sin servidor que te permite conectar servicios de Google Cloud, así como servicios externos, a través de eventos. Su función principal es **detectar eventos** que ocurren en tu infraestructura (como la subida de un archivo a un bucket de GCS) y luego **enrutar esos eventos** a un destino específico para su procesamiento.\n",
        "\n",
        "*   **Detecta eventos:** Cuando se sube un nuevo archivo al bucket, Eventarc lo detecta.\n",
        "*   **No consulta constantemente (No polling):** A diferencia de un proceso que revisa el bucket cada cierto tiempo (polling), Eventarc funciona de manera reactiva. Es decir, no está preguntando activamente si hay algo nuevo; simplemente **escucha** y se activa cuando ocurre un evento específico.\n",
        "*   **Escucha eventos:** Se configura para \"escuchar\" un tipo particular de evento (en nuestro caso, la creación de un nuevo objeto en un bucket de Cloud Storage).\n",
        "\n",
        "Esto introduce una idea fundamental en los sistemas modernos de Big Data:\n",
        "\n",
        "### **Arquitectura Event-Driven (Orientada a Eventos)**\n",
        "\n",
        "Una arquitectura orientada a eventos es un patrón de diseño que promueve la producción, detección, consumo y reacción a eventos. Cuando un evento ocurre, un servicio (o \"productor\") emite una notificación, y otros servicios (o \"consumidores\") que están interesados en ese tipo de evento reaccionan a él. Esto permite una gran flexibilidad y escalabilidad.\n",
        "\n",
        "En una empresa real, esto permite:\n",
        "\n",
        "*   **Automatización:** Los procesos se inician automáticamente sin intervención manual.\n",
        "*   **Escalabilidad:** Los componentes de procesamiento solo se activan cuando hay eventos, escalando según la demanda.\n",
        "*   **Procesamiento Inmediato:** Los datos se procesan tan pronto como llegan, habilitando casos de uso en tiempo real.\n",
        "*   **Desacoplamiento entre componentes:** El sistema que genera el evento (el Data Lake al recibir un archivo) no necesita saber quién lo procesará, y el procesador no necesita saber quién generó el evento. Esto facilita la evolución y el mantenimiento de la arquitectura.\n",
        "\n",
        "**El rol de Eventarc:** Eventarc no transforma los datos. Su función es puramente de **orquestación**: **detecta y dispara** la ejecución de otro componente o servicio en respuesta a un evento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac01e76a"
      },
      "source": [
        "## Cómo Generar una Cloud Function Basada en Eventarc para Clasificación de Archivos\n",
        "\n",
        "Para configurar una Cloud Function que reaccione a la subida de archivos a tu Data Lake (bucket de Cloud Storage) y los clasifique, sigue estos pasos detallados:\n",
        "\n",
        "0.  **Configuración de Permisos en IAM:**\n",
        "\n",
        "    Es fundamental otorgar los permisos necesarios a la cuenta de servicio que utilizará la Cloud Function para interactuar con Cloud Storage. Ingresa a la sección de IAM (Identity and Access Management) en la consola de GCP y asegúrate de que la cuenta de servicio asociada a tu proyecto tenga al menos el rol de **Administrador de objetos de Storage** o permisos equivalentes de lectura y escritura para el bucket.\n",
        "    ![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/5.png)\n",
        "\n",
        "1.  **Navega a Cloud Run (o Cloud Functions):**\n",
        "    En la barra de búsqueda de la consola de GCP, busca \"Cloud Run\" o \"Cloud Functions\" y selecciona el servicio. La configuración que sigue es aplicable a ambos, aunque las pantallas pueden variar ligeramente.\n",
        "\n",
        "2.  **Habilita las APIs Necesarias:**\n",
        "    Asegúrate de que las APIs de **Cloud Run Admin API**, **Cloud Build API**, **Cloud Storage API** y **Eventarc API** estén habilitadas en tu proyecto. El propio proceso de creación suele sugerir su activación si no lo están.\n",
        "\n",
        "3.  **Crear una Nueva Función (Servicio):**\n",
        "    *   Haz clic en \"Crear servicio\" (o \"Crear función\" si estás en Cloud Functions).\n",
        "    *   Selecciona la opción \"Usar un editor intercalado para crear una función\" (o \"Crear desde cero\").\n",
        "    *   **Nombre del servicio:** Asigna un nombre descriptivo, por ejemplo, `classify-files`.\n",
        "    *   **Región:** Elige la misma región donde se encuentra tu bucket de Cloud Storage para minimizar la latencia.\n",
        "    *   **Entorno de ejecución:** Selecciona `Python 3.11` (o la versión de Python que prefieras).\n",
        "    *   **Configuración del Activador (Trigger):**\n",
        "        *   Agrega un activador de tipo **Cloud Storage**.\n",
        "        *   Selecciona tu bucket de Data Lake como la fuente del evento.\n",
        "        *   Elige el tipo de evento: `Finalizar/crear` (cuando se crea un nuevo objeto en el bucket).\n",
        "        *   Selecciona la cuenta de servicio que configuraste en el paso 0.\n",
        "    *   **Configuración de la Facturación y Escalado:**\n",
        "        *   Verifica que la facturación esté \"Basada en solicitudes\".\n",
        "        *   En el escalado automático, puedes configurar un máximo de instancias (ej: `1` para empezar, ajusta según la carga esperada).\n",
        "    *   Confirma la creación del servicio.\n",
        "\n",
        "4.  **Implementa el Código de la Función:**\n",
        "    Una vez que el servicio (o función) esté creado y desplegado inicialmente, dale click y navega a la sección de \"Fuente\" o \"Código fuente\".\n",
        "\n",
        "    *   **En `main.py`, pega el siguiente código:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "def classify_and_route_file(request):\n",
        "    print(\"Headers:\", dict(request.headers))\n",
        "\n",
        "    # Intentar obtener el body como texto\n",
        "    raw_body = request.data.decode(\"utf-8\") if request.data else \"\"\n",
        "    print(\"Raw body:\", raw_body)\n",
        "\n",
        "    # Intentar parsear JSON\n",
        "    try:\n",
        "        event = json.loads(raw_body) if raw_body else {}\n",
        "    except Exception as e:\n",
        "        print(\"Error parsing JSON:\", e)\n",
        "        event = {}\n",
        "\n",
        "    # En modo GCS_NOTIFICATION el body ya es el payload directo\n",
        "    bucket_name = event.get(\"bucket\")\n",
        "    file_name = event.get(\"name\")\n",
        "\n",
        "    if not bucket_name or not file_name:\n",
        "        print(\"No bucket/name in body\")\n",
        "        return \"OK\", 200\n",
        "\n",
        "    if not file_name.startswith(\"raw/\"):\n",
        "        print(\"Not in raw/, ignored\")\n",
        "        return \"OK\", 200\n",
        "\n",
        "    print(f\"Processing {file_name}\")\n",
        "    try:\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(bucket_name)\n",
        "        blob = bucket.blob(file_name)\n",
        "\n",
        "        ext = os.path.splitext(file_name.lower())[1]\n",
        "\n",
        "        if ext in [\".xlsx\", \".xls\"]:\n",
        "            new_path = f\"processed/excel/{file_name.split('/')[-1]}\"\n",
        "        elif ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "            new_path = f\"processed/images/{file_name.split('/')[-1]}\"\n",
        "        else:\n",
        "            new_path = f\"processed/other/{file_name.split('/')[-1]}\"\n",
        "\n",
        "        destination_blob = bucket.blob(new_path)\n",
        "\n",
        "        if destination_blob.exists():\n",
        "            print(\"Ya fue procesado anteriormente.\")\n",
        "            return \"OK\", 200\n",
        "\n",
        "        bucket.copy_blob(blob, bucket, new_path)\n",
        "        bucket.delete_blob(file_name)\n",
        "\n",
        "        print(f\"Movido a {new_path}\")\n",
        "        return \"OK\", 200\n",
        "    except Exception as e:\n",
        "        print(\"ERROR REAL:\", str(e))\n",
        "        return \"OK\", 200  # IMPORTANTE: evitar reintentos infinitos\n",
        "        \n",
        "```\n",
        "\n",
        "*   **En `requirements.txt`, agrega la dependencia necesaria:**\n",
        "\n",
        "```\n",
        "    google-cloud-storage\n",
        "```\n",
        "\n",
        "*   Guarda los cambios y haz clic en \"Implementar\" para que la función se actualice. El proceso de despliegue puede tardar unos minutos."
      ],
      "metadata": {
        "id": "b_Nqy3n2BeT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se generarón 2 cloud functions adicionales para llevar la información a Bigquery\n",
        "\n",
        "### Para el procesamiento de imagenes\n",
        "\n",
        "requirements.txt\n",
        "```\n",
        "google-cloud-storage\n",
        "google-cloud-bigquery\n",
        "google-cloud-vision\n",
        "functions-framework\n",
        "```\n",
        "\n",
        "\n",
        "main.py\n",
        "```\n",
        "import re\n",
        "from datetime import datetime\n",
        "from google.cloud import bigquery, vision\n",
        "import functions_framework\n",
        "\n",
        "PROJECT_ID = \"project-656c27e1-a20c-449d-9bc\"\n",
        "DATASET = \"dw_estudiantes\"\n",
        "TABLE = \"ganancias_extra\"\n",
        "\n",
        "@functions_framework.cloud_event\n",
        "def process_image(cloud_event):\n",
        "    try:\n",
        "        data = cloud_event.data\n",
        "        bucket_name = data[\"bucket\"]\n",
        "        file_name = data[\"name\"]\n",
        "\n",
        "        if not file_name.startswith(\"processed/images/\"):\n",
        "            return \"OK\", 200\n",
        "\n",
        "        # Extraer ID y año del nombre 111_2025.jpg\n",
        "        base_name = file_name.split(\"/\")[-1]\n",
        "        student_id = int(base_name.split(\"_\")[0])\n",
        "        anio = int(base_name.split(\"_\")[1].split(\".\")[0])\n",
        "\n",
        "        vision_client = vision.ImageAnnotatorClient()\n",
        "        image_uri = f\"gs://{bucket_name}/{file_name}\"\n",
        "\n",
        "        response = vision_client.document_text_detection({\n",
        "            \"source\": {\"image_uri\": image_uri}\n",
        "        })\n",
        "\n",
        "        full_text = response.full_text_annotation.text\n",
        "\n",
        "        clean_number = re.sub(r'[^0-9]', '', full_text)\n",
        "\n",
        "        if not clean_number:\n",
        "            return \"OK\", 200\n",
        "\n",
        "        valor_extra = int(clean_number)\n",
        "\n",
        "        bq_client = bigquery.Client()\n",
        "\n",
        "        # MERGE para evitar duplicados\n",
        "        merge_query = f\"\"\"\n",
        "        MERGE `{PROJECT_ID}.{DATASET}.{TABLE}` T\n",
        "        USING (SELECT {student_id} AS ID,\n",
        "                      {anio} AS anio,\n",
        "                      {valor_extra} AS valor_extra,\n",
        "                      CURRENT_TIMESTAMP() AS ingestion_timestamp) S\n",
        "        ON T.ID = S.ID AND T.anio = S.anio\n",
        "        WHEN MATCHED THEN\n",
        "          UPDATE SET\n",
        "            valor_extra = S.valor_extra,\n",
        "            ingestion_timestamp = S.ingestion_timestamp\n",
        "        WHEN NOT MATCHED THEN\n",
        "          INSERT ROW\n",
        "        \"\"\"\n",
        "\n",
        "        bq_client.query(merge_query).result()\n",
        "\n",
        "        print(\"Ganancia guardada en tabla independiente\")\n",
        "\n",
        "        return \"OK\", 200\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", str(e))\n",
        "        return \"OK\", 200\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0HRSZEbQCMqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PAra la extracción de excel\n",
        "\n",
        "requirements.txt\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "google-cloud-storage\n",
        "google-cloud-bigquery\n",
        "pandas\n",
        "openpyxl\n",
        "pyarrow\n",
        "functions-framework\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "main.py\n",
        "```\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "import functions_framework\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ID = \"project-656c27e1-a20c-449d-9bc\"\n",
        "DATASET = \"dw_estudiantes\"\n",
        "TABLE = \"raw_estudiantes\"\n",
        "\n",
        "@functions_framework.cloud_event\n",
        "def load_excel_to_bq(cloud_event):\n",
        "    try:\n",
        "        data = cloud_event.data\n",
        "        bucket_name = data[\"bucket\"]\n",
        "        file_name = data[\"name\"]\n",
        "\n",
        "        if not file_name.startswith(\"processed/excel/\"):\n",
        "            return \"OK\", 200\n",
        "\n",
        "        print(f\"Procesando Excel: {file_name}\")\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(file_name)\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".xlsx\") as temp:\n",
        "            blob.download_to_filename(temp.name)\n",
        "            df = pd.read_excel(temp.name)\n",
        "\n",
        "        # Validar columnas\n",
        "        required_cols = [\"ID\",\"Nombre\",\"Edad\",\"Salario\",\"Gastos\",\"Sexo\",\"año\"]\n",
        "        for col in required_cols:\n",
        "            if col not in df.columns:\n",
        "                raise Exception(f\"Falta columna {col}\")\n",
        "\n",
        "        df[\"anio\"] = df[\"año\"]\n",
        "        df.drop(columns=[\"año\"], inplace=True)\n",
        "\n",
        "        df[\"Ganancias_adicionales\"] = None\n",
        "        df[\"ingestion_timestamp\"] = datetime.utcnow()\n",
        "\n",
        "        bq_client = bigquery.Client()\n",
        "\n",
        "        job = bq_client.load_table_from_dataframe(\n",
        "            df,\n",
        "            f\"{PROJECT_ID}.{DATASET}.{TABLE}\"\n",
        "        )\n",
        "\n",
        "        job.result()\n",
        "\n",
        "        print(\"Carga a BigQuery completada\")\n",
        "\n",
        "        return \"OK\", 200\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", str(e))\n",
        "        return \"OK\", 200\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Hdu6pbpwCo4c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "40fG_KtW9eYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a240942e"
      },
      "source": [
        "## ¿Qué es Google BigQuery?\n",
        "\n",
        "**Google BigQuery** es un **data warehouse (almacén de datos) empresarial completamente administrado y sin servidor** que permite analizar terabytes y petabytes de datos de manera súper rápida utilizando consultas tipo SQL. Es la columna vertebral de muchas arquitecturas de Big Data en la nube de Google.\n",
        "\n",
        "### Características Clave de BigQuery:\n",
        "\n",
        "*   **Sin Servidor (Serverless):** No tienes que gestionar ninguna infraestructura. Google se encarga de todo el aprovisionamiento, escalado y mantenimiento de los servidores.\n",
        "*   **Escalabilidad Masiva:** Puede escalar para almacenar y analizar conjuntos de datos de cualquier tamaño, desde gigabytes hasta petabytes, sin afectar el rendimiento.\n",
        "*   **Rendimiento Analítico Superior:** Está optimizado para consultas analíticas complejas sobre grandes volúmenes de datos. Utiliza una arquitectura columnar que permite leer solo los datos relevantes para una consulta, lo que resulta en una velocidad impresionante.\n",
        "*   **SQL Estándar:** Permite usar SQL estándar (ANSI SQL 2011) para consultar los datos, lo que lo hace accesible para cualquier persona familiarizada con bases de datos relacionales.\n",
        "*   **Almacenamiento y Cómputo Separados:** Los recursos de almacenamiento y cómputo están desacoplados, lo que permite escalarlos de forma independiente y optimizar costos.\n",
        "*   **Integración Nativa:** Se integra fácilmente con otras herramientas de Google Cloud, como Cloud Storage (Data Lake), Cloud Functions, Looker Studio (para visualizaciones), y más.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e15de8bb"
      },
      "source": [
        "## ¿Qué hacen las funciones que llevan del Data Lake a BigQuery?\n",
        "\n",
        "Las Cloud Functions (ej. las que se activan con Cloud Run) que operan en esta fase cumplen una tarea fundamental: transforman los datos de su estado crudo en el Data Lake a un formato estructurado y optimizado para el análisis en BigQuery. Esencialmente, realizan tres pasos clave:\n",
        "\n",
        "1.  **Leen el archivo del Data Lake:** Acceden al objeto de datos (tu `.xlsx` o la imagen `.png`) desde el bucket de Cloud Storage.\n",
        "2.  **Interpretan su contenido:** Procesan el archivo; por ejemplo, leen las filas y columnas de un Excel o extraen texto de una imagen.\n",
        "3.  **Lo insertan como tabla estructurada en BigQuery:** Este es el punto arquitectónico más importante, ya que cambia fundamentalmente la naturaleza de nuestros datos.\n",
        "\n",
        "### El Paso Arquitectónico Crucial: De Objeto a Tabla en BigQuery\n",
        "\n",
        "Cuando los datos son insertados en BigQuery, ocurre una transformación conceptual vital:\n",
        "\n",
        "*   **El archivo deja de ser un objeto:** Ya no es solo un archivo `.xlsx` o `.png` genérico. Su contenido ha sido parseado e interpretado.\n",
        "*   **Se convierte en tabla:** Los datos ahora residen en filas y columnas, con un formato relacional o tabular.\n",
        "*   **Tiene esquema:** A cada columna se le asigna un tipo de dato (`INTEGER`, `STRING`, `TIMESTAMP`, etc.). Esto impone estructura y garantiza la integridad de los datos para futuras consultas.\n",
        "*   **Puede ser consultado con SQL:** BigQuery es una base de datos analítica que permite el uso de SQL estándar para consultar, filtrar, agrupar y agregar datos de manera eficiente. Aquí es donde realmente entramos en el mundo **OLAP (Online Analytical Processing)**.\n",
        "\n",
        "### ¿Eso es ETL o ELT?\n",
        "\n",
        "La distinción entre **ETL (Extract, Transform, Load)** y **ELT (Extract, Load, Transform)** radica en el momento y lugar de la fase de `Transformación`. La elección entre uno u otro depende de la estrategia de la arquitectura de datos y las capacidades de las herramientas:\n",
        "\n",
        "*   **ETL (Extract → Transform → Load):**\n",
        "    Si la función de Cloud Run (o cualquier otro motor de procesamiento intermedio) realiza una limpieza exhaustiva, calcula métricas, aplica reglas de negocio complejas y *luego* inserta los datos ya listos y transformados en BigQuery, entonces estamos ante un proceso ETL.\n",
        "\n",
        "*   **ELT (Extract → Load → Transform):**\n",
        "    Si la función de Cloud Run se limita a extraer los datos del Data Lake, cargarlos *casi crudos* en BigQuery (quizás solo con una mínima interpretación de formato), y luego las transformaciones complejas (limpieza, cálculos, agregaciones) se realizan *dentro de BigQuery* utilizando consultas SQL, entonces es un proceso ELT.\n",
        "\n",
        "En tu flujo, cuando la función carga los datos en tablas como `raw_estudiantes` en BigQuery, y posteriormente se utilizan consultas SQL dentro de BigQuery para crear tablas más elaboradas como `fact_finanzas`, estás realizando un enfoque **ELT**. Esto aprovecha la potencia de BigQuery para las transformaciones a gran escala.\n",
        "\n",
        "### DataLAke a DataWArehousw\n",
        "\n",
        "La transición del Data Lake a BigQuery (como Data Warehouse) representa una evolución en la utilidad del dato:\n",
        "\n",
        "*   **Data Lake:** Es el almacenamiento masivo y flexible de todos los datos en su forma original. Es el \"estanque\" donde todo se guarda.\n",
        "*   **Data Warehouse (BigQuery en este caso):** Es el almacenamiento optimizado para análisis estructurados. Es la \"biblioteca\" organizada para la consulta rápida y la generación de valor.\n",
        "\n",
        "El paso del **Lake a Warehouse** implica:\n",
        "\n",
        "*   **Estructuración:** Los datos, que en el Data Lake no tenían un esquema formal, ahora lo adquieren.\n",
        "*   **Optimización para Consultas:** Mientras que un archivo en Cloud Storage no tiene índices, no está particionado y no está optimizado para consultas analíticas complejas, cuando entra a BigQuery...\n",
        "\n",
        "    *   Se convierte en una **tabla columnar**, lo que acelera enormemente las consultas analíticas.\n",
        "    *   Puede **particionarse** y **agruparse** por columnas clave para mejorar el rendimiento y reducir costos.\n",
        "    *   Puede **agregarse** fácilmente para resumir información.\n",
        "    *   Está listo para **alimentar dashboards** e informes de Business Intelligence.\n",
        "\n",
        "En este punto, ya estamos en la **capa analítica**, donde los datos brutos se han convertido en información procesable.\n",
        "\n",
        "### ¿Cómo se lleva esto a un Data Warehouse (BigQuery)?\n",
        "\n",
        "El proceso típico es:\n",
        "\n",
        "1.  **El archivo crudo vive en el Data Lake.**\n",
        "2.  **Una función (Cloud Run)** lo procesa, extrayendo el contenido.\n",
        "3.  **Se define un esquema** (implícito o explícito) para esos datos.\n",
        "4.  **Se carga en BigQuery** (en lo que se suele llamar una \"zona staging\" o \"raw\"). Por ejemplo, la tabla `raw_estudiantes`.\n",
        "5.  **Luego, dentro de BigQuery, se crean tablas transformadas** (como `fact_finanzas` o `dim_estudiantes`) aplicando reglas de negocio, uniones y agregaciones mediante SQL.\n",
        "\n",
        "Es crucial entender que el **Data Warehouse no reemplaza al Data Lake**. Conviven y se complementan:\n",
        "\n",
        "*   **Data Lake:** Guarda todo, incluyendo datos crudos, históricos completos y una flexibilidad inigualable para futuros usos desconocidos.\n",
        "*   **Data Warehouse:** Guarda datos estructurados, optimizados para análisis específicos y diseñados para las necesidades del negocio. Proporciona un rendimiento superior para consultas predefinidas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwWOdUabqMkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnWARCduqpmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebedfd44"
      },
      "source": [
        "Para crear las tablas en BigQuery, puedes usar las siguientes sentencias SQL:\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE dw_estudiantes.raw_estudiantes (\n",
        "  ID INT64,\n",
        "  Nombre STRING,\n",
        "  Edad INT64,\n",
        "  Salario INT64,\n",
        "  Gastos INT64,\n",
        "  Sexo STRING,\n",
        "  anio INT64,\n",
        "  ingestion_timestamp TIMESTAMP\n",
        ");\n",
        "```\n",
        "\n",
        "y\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE dw_estudiantes.ganancias_extra (\n",
        "  ID INT64,\n",
        "  anio INT64,\n",
        "  valor_extra INT64,\n",
        "  ingestion_timestamp TIMESTAMP\n",
        ");\n",
        "```\n",
        "Ahora tenemos dos fuentes estructuradas dentro del Warehouse."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "971y6Haip1nw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ef0c42"
      },
      "source": [
        "## Transformación en BigQuery: Creación de `fact_finanzas`\n",
        "\n",
        "En esta etapa crucial, realizamos la fase de **Transformación (T)** del proceso ELT, ejecutándola directamente **dentro de BigQuery**. A diferencia de las fases anteriores donde el procesamiento podría ser gestionado por servicios externos, aquí aprovechamos la capacidad analítica inherente de BigQuery para manipular y consolidar los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iAGHfDOXEPMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26e619db"
      },
      "source": [
        "## Modelos Dimensionales (Data Warehousing)\n",
        "\n",
        "Los modelos dimensionales son un tipo de modelado de datos utilizado principalmente en entornos de **Data Warehousing** para análisis y Business Intelligence. Están optimizados para la lectura, consulta y agregación de grandes volúmenes de datos, en contraste con los modelos relacionales que se optimizan para transacciones (escritura).\n",
        "\n",
        "### 1. Modelo Estrella (Star Schema)\n",
        "\n",
        "*   **Descripción:** Es el modelo dimensional más simple y común. Su nombre se debe a su apariencia gráfica: una **tabla de hechos central** rodeada directamente por varias **tablas de dimensión**.\n",
        "*   **Componentes Clave:**\n",
        "    *   **Tabla de Hechos (Fact Table):** Contiene las **medidas cuantitativas** (métricas) que se desean analizar (ej. `ventas_totales`, `cantidad_productos`, `utilidad`). También contiene las **claves foráneas** que enlazan con las tablas de dimensión.\n",
        "    *   **Tablas de Dimensión (Dimension Tables):** Proporcionan el **contexto descriptivo** (información cualitativa) para los hechos. Responden a las preguntas 'quién', 'qué', 'cuándo', 'dónde', 'cómo'. Están **desnormalizadas**, lo que significa que agrupan todos los atributos de una entidad en una sola tabla para simplificar las uniones.\n",
        "*   **Ventajas:**\n",
        "    *   **Simplicidad:** Fácil de entender e implementar. Los usuarios de negocio pueden navegarlo fácilmente.\n",
        "    *   **Rendimiento en Consultas:** Extremadamente eficiente para consultas analíticas. Las uniones son simples y rápidas porque solo hay un 'salto' de la tabla de hechos a cada dimensión.\n",
        "    *   **Compatibilidad:** La mayoría de las herramientas de Business Intelligence (BI) están diseñadas para trabajar de forma óptima con esquemas estrella.\n",
        "*   **Desventajas:**\n",
        "    *   Puede haber cierta redundancia de datos en las dimensiones (debido a la desnormalización).\n",
        "    *   Menos flexible para cambios complejos en la estructura de las dimensiones.\n",
        "    ![](https://learn.microsoft.com/es-es/power-bi/guidance/media/star-schema/star-schema-example1.png)\n",
        "\n",
        "### 2. Modelo Copo de Nieve (Snowflake Schema)\n",
        "\n",
        "*   **Descripción:** Es una extensión del modelo estrella donde las **tablas de dimensión están normalizadas**. Esto significa que las dimensiones se descomponen en sub-dimensiones más pequeñas y relacionadas, formando una estructura jerárquica que se asemeja a un copo de nieve.\n",
        "*   **Componentes Clave:**\n",
        "    *   **Tabla de Hechos (Fact Table):** Igual que en el modelo estrella.\n",
        "    *   **Tablas de Dimensión (Normalizadas):** Las dimensiones se dividen en tablas más pequeñas para reducir la redundancia de datos. Por ejemplo, una dimensión de 'Producto' podría dividirse en 'Producto', 'Categoría de Producto' y 'Marca de Producto'.\n",
        "*   **Ventajas:**\n",
        "    *   **Menor Redundancia de Datos:** La normalización reduce la duplicidad de información, lo que puede ahorrar espacio de almacenamiento, especialmente con dimensiones muy grandes.\n",
        "    *   **Mayor Flexibilidad:** Puede ser más fácil de mantener y modificar en caso de cambios en la estructura de las dimensiones.\n",
        "*   **Desventajas:**\n",
        "    *   **Mayor Complejidad en las Consultas:** Las consultas analíticas requieren más uniones (JOINs) para obtener la información completa, lo que puede afectar negativamente el rendimiento en comparación con el modelo estrella.\n",
        "    *   Más difícil de entender y navegar para usuarios no técnicos.\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/2/2c/Esquema_en_copo_de_nieve.png)\n",
        "### ¿Cuál elegir?\n",
        "\n",
        "La elección entre un modelo estrella y un copo de nieve a menudo se reduce a un equilibrio entre **rendimiento de consulta** (favoreciendo al estrella) y **reducción de redundancia/mantenimiento** (favoreciendo al copo de nieve). En la práctica, el modelo estrella es el preferido en la mayoría de los casos por su simplicidad y excelente rendimiento analítico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95f73066"
      },
      "source": [
        "## Construcción del Modelo Estrella en BigQuery: `fact_finanzas` y Dimensiones\n",
        "\n",
        "En este punto de nuestra arquitectura, nos adentramos en la fase de **Transformación (T)** del proceso ELT, la cual se ejecuta íntegramente **dentro de BigQuery**. Esto nos permite aprovechar la potencia y eficiencia de BigQuery para procesar y estructurar nuestros datos, pasando de un almacenamiento crudo a un modelo analítico formal.\n",
        "\n",
        "### 3. `fact_finanzas` (Tabla de Hechos)\n",
        "\n",
        "Esta tabla representa el núcleo de nuestro modelo dimensional. Su objetivo es consolidar las métricas financieras clave de nuestros estudiantes. La `fact_finanzas` se construye a partir de las tablas `raw_estudiantes` y `ganancias_extra`.\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE dw_estudiantes.fact_finanzas AS\n",
        "SELECT\n",
        "  e.ID,\n",
        "  e.anio,\n",
        "  e.Salario,\n",
        "  e.Gastos,\n",
        "  IFNULL(g.valor_extra,0) AS Ganancias_adicionales,\n",
        "  e.Salario + IFNULL(g.valor_extra,0) AS ingreso_total,\n",
        "  (e.Salario + IFNULL(g.valor_extra,0)) - e.Gastos AS utilidad\n",
        "FROM dw_estudiantes.raw_estudiantes e\n",
        "LEFT JOIN dw_estudiantes.ganancias_extra g\n",
        "  ON e.ID = g.ID\n",
        "  AND e.anio = g.anio;\n",
        "```\n",
        "\n",
        "**Propósito:** La `fact_finanzas` almacena las **medidas cuantitativas** (salario, gastos, ganancias adicionales, ingreso total, utilidad) que son objeto de nuestro análisis. Cada registro en esta tabla representa un 'hecho' financiero específico para un estudiante en un año determinado. Este paso marca la transición de un almacenamiento pasivo a un modelado analítico activo, donde los datos brutos se convierten en información procesable.\n",
        "\n",
        "### 4. Dimensiones: `dim_estudiante` y `dim_tiempo`\n",
        "\n",
        "Para dotar de contexto a los hechos financieros contenidos en `fact_finanzas`, creamos **tablas de dimensión**. Estas tablas proveen los atributos descriptivos que nos permiten entender 'quién' y 'cuándo' ocurrieron los hechos. La combinación de la tabla de hechos con estas dimensiones configura nuestro **Modelo Estrella**.\n",
        "\n",
        "#### `dim_estudiante` (Dimensión de Estudiante)\n",
        "\n",
        "Esta tabla contiene los atributos cualitativos de los estudiantes, extraídos de la tabla `raw_estudiantes`.\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE dw_estudiantes.dim_estudiante AS\n",
        "SELECT DISTINCT\n",
        "  ID,\n",
        "  Nombre,\n",
        "  Edad,\n",
        "  Sexo\n",
        "FROM dw_estudiantes.raw_estudiantes;\n",
        "```\n",
        "\n",
        "**Propósito:** `dim_estudiante` proporciona el **contexto descriptivo** sobre la entidad 'estudiante'. Permite realizar análisis segmentados por nombre, edad o sexo, dándole sentido a las cifras financieras.\n",
        "\n",
        "#### `dim_tiempo` (Dimensión de Tiempo)\n",
        "\n",
        "Esta tabla agrupa los componentes temporales relevantes para nuestros análisis, en este caso el año, también derivado de `raw_estudiantes`.\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE dw_estudiantes.dim_tiempo AS\n",
        "SELECT DISTINCT\n",
        "  anio\n",
        "FROM dw_estudiantes.raw_estudiantes;\n",
        "```\n",
        "\n",
        "**Propósito:** `dim_tiempo` ofrece el **contexto temporal** de los hechos financieros. Facilita el análisis de tendencias y comparaciones a lo largo del tiempo, utilizando el año como principal atributo.\n",
        "\n",
        "### La Estructura del Modelo Estrella:\n",
        "\n",
        "Con estas tablas, hemos implementado un **Modelo Estrella** sólido:\n",
        "\n",
        "*   La tabla `fact_finanzas` actúa como la **tabla de hechos central**.\n",
        "*   Las tablas `dim_estudiante` y `dim_tiempo` son las **tablas de dimensión** que se conectan directamente a la tabla de hechos a través de sus respectivas claves. Estas dimensiones 'irradian' desde la tabla de hechos, creando la forma de una estrella.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12cd39cd"
      },
      "source": [
        "## Creación de un Data Mart Formal\n",
        "\n",
        "Una vez que hemos construido nuestro Data Warehouse con tablas de hechos y dimensiones, el siguiente paso lógico es la creación de un **Data Mart**. Un Data Mart es un subconjunto de datos de un Data Warehouse, diseñado para servir a un departamento o función de negocio específica (en este caso, finanzas de estudiantes).\n",
        "\n",
        "En lugar de que las herramientas de Business Intelligence (BI) accedan directamente a todas las tablas complejas del Data Warehouse, creamos una vista o tabla dedicada que simplifica el modelo y expone solo la información relevante para un análisis específico.\n",
        "\n",
        "### Ejemplo: Creación de la Vista `mart_finanzas`\n",
        "\n",
        "Creamos una vista (`VIEW`) llamada `mart_finanzas` que combina información de nuestra tabla de hechos (`fact_finanzas`) con atributos de la dimensión de estudiantes (`dim_estudiante`).\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE VIEW dw_estudiantes.mart_finanzas AS\n",
        "SELECT\n",
        "  f.ID,\n",
        "  d.Nombre,\n",
        "  d.Sexo,\n",
        "  f.anio,\n",
        "  f.ingreso_total,\n",
        "  f.utilidad\n",
        "FROM dw_estudiantes.fact_finanzas f\n",
        "JOIN dw_estudiantes.dim_estudiante d\n",
        "  ON f.ID = d.ID;\n",
        "```\n",
        "\n",
        "### ¿Por qué utilizar un Data Mart?\n",
        "\n",
        "La creación de un Data Mart, como `mart_finanzas`, ofrece varias ventajas clave:\n",
        "\n",
        "1.  **Simplifica el Modelo:** Las herramientas de BI, como Looker, necesitan modelos de datos sencillos y de alto rendimiento. Un Data Mart presenta una vista plana y optimizada que es fácil de entender y de consultar para los analistas.\n",
        "2.  **Controla el Acceso:** Permite gestionar permisos de acceso de forma más granular. Un usuario o una herramienta de BI pueden tener acceso solo al Data Mart relevante, sin necesidad de acceder a todo el Data Warehouse.\n",
        "3.  **Expone Solo lo Necesario:** Se selecciona y pre-calcula únicamente la información requerida para los informes y dashboards específicos del área de negocio. Esto reduce la complejidad y mejora el rendimiento al evitar que las herramientas de BI carguen datos innecesarios.\n",
        "4.  **Estabilidad y Consistencia:** Un Data Mart proporciona una fuente de datos estable y consistente para los usuarios finales. Los cambios en la estructura interna del Data Warehouse pueden manejarse sin afectar directamente a los informes y aplicaciones que dependen del Data Mart.\n",
        "\n",
        "En resumen, el Data Mart actúa como una capa de presentación optimizada entre el Data Warehouse y las herramientas de BI, facilitando el análisis y la toma de decisiones empresariales."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flujo Completo del Dato\n",
        "```\n",
        "Drive (OLTP)\n",
        "      ↓\n",
        "Apps Script (Ingesta)\n",
        "      ↓\n",
        "Cloud Storage (Data Lake)\n",
        "      ↓\n",
        "Eventarc (Evento)\n",
        "      ↓\n",
        "Cloud Run (Procesamiento)\n",
        "      ↓\n",
        "BigQuery (Data Warehouse OLAP)\n",
        "      ↓\n",
        "BigQuery ML (Predicción)\n",
        "      ↓\n",
        "Looker Studio (BI)\n",
        "```"
      ],
      "metadata": {
        "id": "I4qg_3k1fay9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yDP5YpbofQV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}