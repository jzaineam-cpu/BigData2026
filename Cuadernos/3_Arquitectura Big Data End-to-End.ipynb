{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU1kaJ6M15UU5DPbOa80ZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazaineam1/BigData2026/blob/main/Cuadernos/3_Arquitectura%20Big%20Data%20End-to-End.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Arquitectura Empresarial y Ciclo de vida de los Datos***\n",
        "\n",
        "## ***Universidad Central***\n",
        ">## **Facultad de Ingeniería y Ciencias Básicas.**\n",
        ">## ***Maestría en analítica de datos***\n",
        "![Imágen1](https://www.ucentral.edu.co/sites/default/files/logo_1.png)\n",
        "\n",
        "\n",
        ">## ***Big Data.***\n",
        ">## ***Docente: Antonino Zainea Maya.***"
      ],
      "metadata": {
        "id": "-JlEuwKXldMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aates de comenzar ...\n",
        "¿Dónde nacen los datos dentro de una organización?\n",
        "\n",
        "¿Todos los datos son estructurados?\n",
        "\n",
        "¿Es lo mismo almacenar que analizar?\n",
        "\n",
        "¿Puede una empresa hacer análisis directamente sobre su sistema transaccional?\n",
        "\n",
        "¿Qué problemas surgirían si no existiera separación entre sistemas operativos y analíticos?"
      ],
      "metadata": {
        "id": "o2DvdwCbmQI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio Inicial — Simulación del Nacimiento del Dato\n",
        "> 1. Crea un archivo .xlsx\n",
        ">2. Crear las siguientes columnas:\n",
        "\n",
        "|ID | Nombre | Edad | Salario | Gastos | año|\n",
        "|---|---|---|---|---|---|\n",
        "\n",
        ">3. Agregar un registro (datos ficticios), como sigue.\n",
        "\n",
        "|ID | Nombre | Edad | Salario | Gastos | año|\n",
        "|---|---|---|---|---|---|\n",
        "|111 | Ana | 28 | 4000000 | 2500000 | 2024|\n",
        "\n",
        "> 4. Guardar el archivo como `ID_año.xlsx`, en nuestro ejemplo:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "111_2024.xlsx\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "> 5. Crear una imagen simple escribir únicamente un número grande (ejemplo: 1500000). Ese número representará una “ganancia adicional”.\n",
        "> 6. Guardar la imagen como `ID_año.png` o con el formato adecuado, en nuestro ejemplo:\n",
        "\n",
        "```\n",
        "111_2024.png\n",
        "```\n",
        "\n",
        "> 7. Carga la información en [https://drive.google.com/drive/folders/1F7t1Yyep4sEl9D9QjDzVWTwP6r4o1LiL?usp=sharing](https://drive.google.com/drive/folders/1F7t1Yyep4sEl9D9QjDzVWTwP6r4o1LiL?usp=sharing)\n",
        "\n",
        "![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/1.png)"
      ],
      "metadata": {
        "id": "z93aEpkDmdOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ol92sx_r6Dq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d060abc"
      },
      "source": [
        "## ¿Por qué comenzamos con un archivo `XLSX`?\n",
        "\n",
        "El ejercicio de crear un archivo `.xlsx` con registros individuales como `ID`, `Nombre`, `Edad`, `Salario`, `Gastos` y `Año`, y luego guardar una imagen con una \"ganancia adicional\", **simula de forma simplificada el nacimiento de los datos en un sistema transaccional.**\n",
        "<img src=\"\" alt=\"\" width=\"50\"/>\n",
        "En una empresa real, estos datos no nacerían en un archivo de Excel, sino en un **Sistema de Procesamiento de Transacciones Online (OLTP por sus siglas en inglés: Online Transaction Processing)**.\n",
        "\n",
        "### **¿Qué es un Sistema OLTP (Online Transaction Processing)?**\n",
        "\n",
        "Un sistema OLTP está diseñado para capturar y procesar transacciones de datos en tiempo real, donde cada interacción, cada venta, cada registro de cliente, cada movimiento de inventario se registra instantáneamente.\n",
        "\n",
        "**Características clave de un sistema OLTP:**\n",
        "\n",
        "*   **Orientado a Transacciones:** Su función principal es registrar y gestionar operaciones individuales de forma eficiente.\n",
        "*   **Rapidez en Escritura (INSERT, UPDATE, DELETE):** Están optimizados para ejecutar rápidamente pequeñas transacciones que insertan, actualizan o eliminan registros.\n",
        "*   **Consistencia y Atomicidad:** Aseguran que cada transacción se complete por completo o no se realice en absoluto (propiedad ACID: Atomicidad, Consistencia, Aislamiento, Durabilidad).\n",
        "*   **Alta Concurrencia:** Permiten que miles o millones de usuarios realicen transacciones simultáneamente sin conflictos.\n",
        "*   **Datos Actuales y Detallados:** Contienen los datos más recientes y a nivel de detalle más bajo, representando el estado actual del negocio.\n",
        "*   **Estructura Rígida y Normalizada:** Suelen tener esquemas de base de datos muy normalizados para reducir la redundancia y asegurar la integridad de los datos.\n",
        "\n",
        "Todas estas operaciones se realizan en milisegundos, garantizando la consistencia de los datos en tiempo real. **El archivo XLSX del ejercicio, con su registro individual y la imagen de \"ganancia adicional\", simula esta naturaleza de datos atómicos y actuales que se generan constantemente en un OLTP.**\n",
        "\n",
        "\n",
        "En una empresa real, ese archivo no existiría como xlsx.\n",
        "Existiría como:\n",
        "\n",
        "- Base de datos\n",
        "- CRM (Customer Relationship Management)\n",
        "\n",
        "### **El Problema Empresarial: ¿Por qué no analizar directamente en un OLTP?**\n",
        "\n",
        "Aunque los sistemas OLTP son excelentes para la operación diaria, **NO están optimizados para el análisis de datos complejos o históricos.** Aquí radica el \"problema empresarial\":\n",
        "\n",
        "1.  **Impacto en el Rendimiento Operacional:**\n",
        "2.  **No Optimizado para Agregaciones y Tendencias:**\n",
        "3.  **Datos Actuales vs. Históricos:**\n",
        "\n",
        "## Centralización → Data Lake (donde guardaríamos xlsx + imagen)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9197b6d"
      },
      "source": [
        "\n",
        "Un **Data Lake** es un repositorio centralizado y escalable, fundamental en arquitecturas de Big Data, donde se almacenan grandes volúmenes de datos en su **forma original y sin procesar**. Esto es clave: no exige una transformación inmediata ni la imposición de un esquema rígido en el momento de la ingesta (conocido como _schema-on-read_).\n",
        "\n",
        "Esto implica que un Data Lake puede:\n",
        "\n",
        "*   **Almacenar datos de cualquier tipo:** Desde archivos estructurados (como CSV, XLSX, datos de bases de datos relacionales), semiestructurados (JSON, XML), hasta no estructurados (imágenes, audios, videos, PDFs, logs, correos electrónicos). La flexibilidad es su gran fortaleza.\n",
        "*   **Retener datos en su estado crudo:** No es necesario limpiar, transformar o definir la estructura de los datos antes de guardarlos. Esto reduce el tiempo y el esfuerzo inicial, y permite que los datos se utilicen para diversos propósitos futuros sin limitaciones por una estructura predefinida.\n",
        "*   **Escalar de forma masiva y rentable:** Están diseñados para manejar volúmenes de datos que van desde terabytes hasta petabytes, con costos de almacenamiento generalmente más bajos que los sistemas tradicionales de bases de datos.\n",
        "\n",
        "Es fundamental comprender las **funciones y limitaciones de un Data Lake en el contexto de Big Data**:\n",
        "\n",
        "*   El Data Lake **no analiza** los datos por sí mismo. Su propósito no es realizar consultas complejas o generar informes directos.\n",
        "*   El Data Lake **no transforma** activamente los datos. Si bien puede albergar herramientas para transformación (ETL/ELT), no es su función intrínseca modificarlos al almacenar.\n",
        "*   Su rol principal es el **almacenamiento flexible, masivo y la centralización** de datos brutos.\n",
        "\n",
        "### **¿Cómo se refleja el Data Lake en nuestro ejercicio?**\n",
        "\n",
        "En el ejercicio práctico, ustedes crearon:\n",
        "\n",
        "*   Un archivo `.xlsx` (representando un dato estructurado originado en un OLTP).\n",
        "*   Una imagen `.png` con un número (representando un dato no estructurado , como una \"ganancia adicional\").\n",
        "\n",
        "Si ahora subimos esos archivos a un servicio de almacenamiento en la nube, como un **bucket de Cloud Storage**, **ese bucket se convierte en nuestro Data Lake de simulación.**\n",
        "\n",
        "**¿Por qué este bucket actúa como nuestro Data Lake?**\n",
        "\n",
        "*   **Centralización:** Está reuniendo y almacenando archivos de múltiples fuentes (en este caso, de múltiples estudiantes/simulaciones de OLTP).\n",
        "*   **Almacenamiento de Datos en su Forma Original:** Guarda el `.xlsx` tal como lo crearon y la imagen `.png` sin procesarla ni modificarla.\n",
        "*   **Sin Transformación ni Validación Inmediata:** No está transformando los datos ni validando reglas de negocio al momento de la ingesta. Simplemente los guarda.\n",
        "*   **Escalabilidad Implícita:** Los servicios de almacenamiento en la nube están inherentemente diseñados para escalar y almacenar grandes volúmenes de datos.\n",
        "\n",
        "En este punto del flujo de datos, el Data Lake **no se preocupa si el `.xlsx` tiene errores, si los datos son consistentes, o si la imagen es legible en términos de negocio**. Su función es puramente de **conservación** de los datos crudos.\n",
        "\n",
        "**En términos del Ciclo de Vida de los Datos:**\n",
        "\n",
        "1.  **Nacimiento (Generación):** Ocurre en el OLTP (simulado por la creación del archivo `.xlsx` por el estudiante y la imagen).\n",
        "2.  **Centralización (Ingesta):** Los datos se mueven al Data Lake (simulado por la subida de los archivos al bucket de Cloud Storage/Drive).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuestro Datalake (Cloud storage)\n",
        "\n",
        "<img src=\"https://blog.nashtechglobal.com/wp-content/uploads/2023/09/Google-Cloud-Storage-Reviews-1024x512-20200419.png\" alt=\"Excel Icon\" width=\"300\"/>\n",
        "\n",
        "[Documentación](https://cloud.google.com/learn/what-is-cloud-storage?hl=es)\n",
        "\n",
        "[**Buckets**](https://docs.cloud.google.com/storage/docs/buckets?hl=es-419)\n",
        "\n",
        "Los buckets son los contenedores básicos que conservan tus datos como objetos. Todo lo que almacenes en Cloud Storage debe estar contenido en un bucket. Puedes usar buckets para organizar tus datos y controlar el acceso a ellos, pero, a diferencia de los directorios y las carpetas, no puedes anidar los buckets."
      ],
      "metadata": {
        "id": "UJ4iIvbs3amn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como crear un bucket\n",
        "1. ingresa a gcp concole.google.com\n",
        "2. busca cloud storage\n",
        "3. da click en crea bucket\n",
        "4. selecciona tu opcion preferida para el bucket (da una breve explicacion)\n",
        "5. al final tendras algo como esto ![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/4.png)"
      ],
      "metadata": {
        "id": "thjakIQV4lej"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ac541dd"
      },
      "source": [
        "### **Cómo crear un Bucket en Google Cloud Storage**\n",
        "\n",
        "Para crear un contenedor de datos (bucket) en Google Cloud Storage, sigue estos pasos:\n",
        "\n",
        "1.  **Accede a la Consola de GCP:** Ingresa a [console.cloud.google.com](https://console.cloud.google.com/) con tu cuenta de Google Cloud.\n",
        "2.  **Navega a Cloud Storage:** En la barra de búsqueda superior, escribe \"Cloud Storage\" y selecciona el servicio correspondiente.\n",
        "3.  **Crea un Bucket Nuevo:** Haz clic en el botón \"Crear bucket\".\n",
        "4.  **Configura los Detalles del Bucket:** Se te presentarán varias opciones importantes:\n",
        "    *   **Nombre del bucket:** Debe ser globalmente único (como un dominio web), sin espacios y en minúsculas (ej: `mi-datalake-uni-central-2024`).\n",
        "    *   **Tipo de ubicación:**\n",
        "        *   **Región:** Para baja latencia en una ubicación geográfica específica (ej: `southamerica-east1` para São Paulo).\n",
        "        *   **Multirregión:** Para alta disponibilidad y redundancia geográfica (ej: `US` para Estados Unidos o `EUROPE` para Europa).\n",
        "        *   **Doble región:** Combina la baja latencia de dos regiones específicas con alta disponibilidad entre ellas.\n",
        "    *   **Clase de almacenamiento:** Define el costo y la disponibilidad de los datos:\n",
        "        *   **Standard:** Acceso frecuente (ideal para datos en uso activo).\n",
        "        *   **Nearline:** Acceso mensual (para copias de seguridad o archivos que se consultan esporádicamente).\n",
        "        *   **Coldline:** Acceso trimestral (para archivado o datos que se consultan muy rara vez).\n",
        "        *   **Archive:** Acceso anual (para archivado a largo plazo con el menor costo).\n",
        "    *   **Control de acceso:** Decide cómo se gestionarán los permisos.\n",
        "        *   **Uniforme:** Simplifica la gestión de accesos al usar solo IAM (Identity and Access Management) para todo el bucket.\n",
        "        *   **Fino:** Permite el control de accesos tanto con IAM como con Listas de Control de Acceso (ACLs) para objetos individuales (más complejo, pero granular).\n",
        "5.  **Revisa y Confirma:** Al final, tendrás un bucket configurado, listo para almacenar tus objetos. Visualmente, se verá similar a esto:\n",
        "\n",
        "    ![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/4.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yBA8uvkjzMmc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039b3508"
      },
      "source": [
        "## Cloud Run — ¿Qué papel cumple?\n",
        "\n",
        "Una vez que Eventarc detecta la llegada de un nuevo archivo a nuestro Data Lake, su siguiente paso es **activar** un servicio que se encargará del procesamiento. En nuestro caso, ese servicio es **Cloud Run**.\n",
        "\n",
        "### **¿Qué es Cloud Run?**\n",
        "\n",
        "**Cloud Run** es una plataforma de cómputo **serverless** que te permite ejecutar contenedores sin estado a través de solicitudes web o eventos. Es la capa de procesamiento ideal para nuestra arquitectura event-driven.\n",
        "\n",
        "Aquí, en Cloud Run, es donde ocurre algo clave en el ciclo de vida de los datos:\n",
        "\n",
        "### **Transformación de Datos**\n",
        "\n",
        "Hasta ahora, nuestro flujo ha sido:\n",
        "\n",
        "1.  **Nacimiento (Generación):** Ocurre en el OLTP (simulado por la creación del archivo `.xlsx` y la imagen).\n",
        "2.  **Centralización (Ingesta):** Los datos se mueven al Data Lake (simulado por la subida de los archivos al bucket de Cloud Storage/Drive).\n",
        "3.  Ahora entramos en → **Procesamiento y Transformación**\n",
        "\n",
        "Cuando Cloud Run es activado por Eventarc tras la llegada de un nuevo archivo al Data Lake, su lógica se encargará de clasificar y procesar esos datos:\n",
        "\n",
        "*   **Si es un archivo Excel (`.xlsx`):** Cloud Run ejecutará un proceso diseñado para extraer, limpiar y transformar los datos estructurados que contiene (ej. validar tipos de datos, calcular nuevos campos, estandarizar formatos). Los datos procesados se almacenarían en otra capa, como un Data Warehouse o un Data Mart.\n",
        "*   **Si es una imagen (`.png`):** Cloud Run activará un proceso para extraer información de la imagen (ej. reconocimiento óptico de caracteres para leer el número, análisis de contenido para metadatos). Los resultados de este procesamiento se almacenarían de forma adecuada, quizás en una base de datos NoSQL o de nuevo en el Data Lake pero en una zona \"procesada\".\n",
        "\n",
        "Esto introduce el concepto fundamental de **separación por tipo de dato** o **procesamiento diferenciado**:\n",
        "\n",
        "*   **Pipelines diferentes para diferentes formatos:** Es común en Big Data tener flujos de procesamiento (pipelines) especializados para datos estructurados, semi-estructurados y no estructurados.\n",
        "*   **Tratamientos distintos según la naturaleza del dato:** Los requisitos de transformación y los algoritmos aplicados varían enormemente entre un archivo de ventas en Excel y una imagen de ganancia. Cloud Run es lo suficientemente flexible para manejar esta diversidad.\n",
        "\n",
        "### **Ventajas de Cloud Run en una Arquitectura Empresarial:**\n",
        "\n",
        "Cloud Run representa la **computación serverless**, lo que significa:\n",
        "\n",
        "*   **No mantiene servidores activos:** Pagas solo por el tiempo que tu código se está ejecutando. No hay servidores que provisionar o gestionar.\n",
        "*   **Se ejecuta bajo demanda:** Se activa solo cuando un evento (como el de Eventarc) lo dispara, o cuando recibe una solicitud web.\n",
        "*   **Escala automáticamente:** Puede escalar desde cero instancias hasta miles en segundos, manejando picos de demanda sin configuración manual.\n",
        "\n",
        "En una empresa grande o en otras plataformas cloud, este rol de \"procesador bajo demanda\" podría ser desempeñado por:\n",
        "\n",
        "*   **Lambda (AWS)**\n",
        "*   **Azure Functions (Microsoft Azure)**\n",
        "*   **Kubernetes (si se ejecuta en un clúster, con un enfoque más de contenedores)**\n",
        "*   **Spark jobs (para procesamiento masivo de lotes, a menudo orquestado por otros servicios)**\n",
        "\n",
        "Pero el rol es el mismo: **ejecutar el código de transformación y procesamiento de datos** que convierte los datos crudos del Data Lake en información valiosa, lista para el análisis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdfa9969"
      },
      "source": [
        "## ¿Qué es Google Apps Script?\n",
        "\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2f/Google_Apps_Script.svg\" alt=\"\" width=\"200\"/>\n",
        "\n",
        "**Google Apps Script (GAS)** es una plataforma de desarrollo basada en la nube que permite extender y automatizar funcionalidades en los productos de Google Workspace (anteriormente G Suite) y más allá. Utiliza JavaScript como lenguaje de programación, lo que lo hace accesible para desarrolladores web y no tan especializados. Se accede y gestiona a través de la URL [script.google.com](https://script.google.com).\n",
        "\n",
        "### Propósito Principal\n",
        "\n",
        "El propósito fundamental de Google Apps Script es **conectar, extender y automatizar** el ecosistema de Google Workspace, facilitando la creación de soluciones personalizadas que mejoren la productividad y eficiencias empresariales. Permite a los usuarios y desarrolladores construir aplicaciones ligeras que interactúan con los datos y servicios de Google de manera programática, sin la necesidad de desplegar o mantener servidores.\n",
        "\n",
        "### Entorno de Ejecución\n",
        "\n",
        "Google Apps Script se ejecuta completamente en la **nube de Google**. Esto significa que los scripts no requieren ninguna infraestructura local por parte del usuario. Los scripts se almacenan en los servidores de Google y se ejecutan desde allí, lo que ofrece las siguientes ventajas:\n",
        "\n",
        "*   **Sin instalación:** No es necesario instalar ningún software o SDK en tu máquina.\n",
        "*   **Accesibilidad:** Puedes acceder y editar tus scripts desde cualquier navegador web.\n",
        "*   **Escalabilidad y Mantenimiento:** Google se encarga de la infraestructura, el escalado y el mantenimiento de los servidores donde se ejecutan tus scripts.\n",
        "\n",
        "### Principales Capacidades\n",
        "\n",
        "Las capacidades de Google Apps Script son amplias y versátiles:\n",
        "\n",
        "1.  **Automatización de Tareas en Google Workspace:**\n",
        "    *   **Hojas de Cálculo (Sheets):** Automatizar informes, manipular datos, crear menús personalizados, enviar alertas basadas en contenido de celdas.\n",
        "    *   **Documentos (Docs):** Generar documentos automáticamente, realizar fusiones de correspondencia, aplicar formato condicional.\n",
        "    *   **Drive:** Organizar archivos, gestionar permisos, crear flujos de trabajo de aprobación.\n",
        "    *   **Gmail:** Enviar correos electrónicos personalizados masivos, filtrar mensajes, crear respuestas automáticas, generar borradores.\n",
        "    *   **Calendario (Calendar):** Programar eventos, sincronizar calendarios, enviar recordatorios.\n",
        "    *   **Formularios (Forms):** Procesar respuestas automáticamente, validar entradas, enviar notificaciones.\n",
        "    *   **Presentaciones (Slides):** Crear presentaciones dinámicas a partir de datos, actualizar contenido.\n",
        "\n",
        "2.  **Integración con Servicios de Google:**\n",
        "    *   Permite interactuar con otras APIs de Google como Google Translate, Google Maps, YouTube, Google Cloud Vision, etc., facilitando la creación de soluciones híbridas y potentes.\n",
        "\n",
        "[Documentación](https://developers.google.com/apps-script/overview?hl=es-419)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si deseas realizar este ejercicio, debes ingresar a [https://script.google.com/](https://script.google.com/)\n",
        "\n",
        "1. Crear un nuevo proyecto llamado `Sync Drive to Cloud Storage`\n",
        "![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/2.png)"
      ],
      "metadata": {
        "id": "6VAzefLP1sMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. en configuración activa\n",
        "Mostrar el archivo de manifiesto \"appsscript.json\" en el editor\n",
        "![](https://raw.githubusercontent.com/jazaineam1/BigData2026/refs/heads/main/Images/Arquitectura/3.png)\n",
        "3. En el editor incluye esta configuración  a appscript.json\n",
        "\n",
        "```\n",
        "{\n",
        "  \"timeZone\": \"America/Bogota\",\n",
        "  \"dependencies\": {\n",
        "  },\n",
        "  \"exceptionLogging\": \"STACKDRIVER\",\n",
        "  \"runtimeVersion\": \"V8\",\n",
        "  \"oauthScopes\": [\n",
        "    \"https://www.googleapis.com/auth/drive.readonly\",\n",
        "    \"https://www.googleapis.com/auth/devstorage.read_write\",\n",
        "    \"https://www.googleapis.com/auth/script.external_request\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "y a codigo.gs\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "/**\n",
        " * ID de la carpeta de Google Drive donde están los archivos de estudiantes\n",
        " */\n",
        "const DRIVE_FOLDER_ID = \"1F7t1Yyep4sEl9D9QjDzVWTwP6r4o1LiL\";\n",
        "\n",
        "/**\n",
        " * Tu nombre de bucket en Cloud Storage\n",
        " */\n",
        "const STORAGE_BUCKET = \"datalake-estudiantes\";\n",
        "\n",
        "/**\n",
        " * Función principal que revisa la carpeta de Drive y copia archivos nuevos a Cloud Storage\n",
        " */\n",
        "function syncDriveFolderToGCS() {\n",
        "  // Obtener la carpeta de Drive\n",
        "  const folder = DriveApp.getFolderById(DRIVE_FOLDER_ID);\n",
        "  \n",
        "  // Lista todos los archivos dentro de esa carpeta\n",
        "  const files = folder.getFiles();\n",
        "  \n",
        "  while (files.hasNext()) {\n",
        "    const file = files.next();\n",
        "    \n",
        "    // Obtener el blob del archivo\n",
        "    const blob = file.getBlob();\n",
        "    \n",
        "    // Crear URL de la API de Cloud Storage\n",
        "    const url = `https://storage.googleapis.com/upload/storage/v1/b/${STORAGE_BUCKET}/o?uploadType=media&name=${encodeURIComponent(\"raw/\" + file.getName())}`;\n",
        "\n",
        "    // Hacer la subida a GCS\n",
        "    const response = UrlFetchApp.fetch(url, {\n",
        "      method: \"POST\",\n",
        "      contentType: blob.getContentType(),\n",
        "      payload: blob.getBytes(),\n",
        "      headers: {\n",
        "        Authorization: \"Bearer \" + ScriptApp.getOAuthToken()\n",
        "      }\n",
        "    });\n",
        "    \n",
        "    console.log(\"Archivo subido:\", file.getName());\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yDysvunI2rQN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNCkQv4_1CZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c98d58"
      },
      "source": [
        "## Eventarc — ¿Por qué aparece ahora?\n",
        "\n",
        "Hasta ahora, nuestro Data Lake (simulado por el bucket de Cloud Storage/Drive) solo cumplía la función de **almacenar** los datos en su forma original. Pero en una arquitectura empresarial real, especialmente en el contexto de Big Data, simplemente almacenar no es suficiente. Necesitamos **reaccionar** de manera automática cuando algo nuevo ocurre, como la llegada de nuevos datos.\n",
        "\n",
        "Pensemos en el escenario de nuestro ejercicio: si 200 estudiantes suben archivos, no es viable ni eficiente que alguien revise manualmente el bucket de Cloud Storage constantemente para ver si hay algo nuevo que procesar.\n",
        "\n",
        "Aquí es donde entra en juego el concepto de **arquitectura orientada a eventos** y, en Google Cloud, el servicio que facilita esto es **Eventarc**.\n",
        "\n",
        "### **¿Qué es Eventarc?**\n",
        "\n",
        "**Eventarc** es una plataforma de eventos sin servidor que te permite conectar servicios de Google Cloud, así como servicios externos, a través de eventos. Su función principal es **detectar eventos** que ocurren en tu infraestructura (como la subida de un archivo a un bucket de GCS) y luego **enrutar esos eventos** a un destino específico para su procesamiento.\n",
        "\n",
        "*   **Detecta eventos:** Cuando se sube un nuevo archivo al bucket, Eventarc lo detecta.\n",
        "*   **No consulta constantemente (No polling):** A diferencia de un proceso que revisa el bucket cada cierto tiempo (polling), Eventarc funciona de manera reactiva. Es decir, no está preguntando activamente si hay algo nuevo; simplemente **escucha** y se activa cuando ocurre un evento específico.\n",
        "*   **Escucha eventos:** Se configura para \"escuchar\" un tipo particular de evento (en nuestro caso, la creación de un nuevo objeto en un bucket de Cloud Storage).\n",
        "\n",
        "Esto introduce una idea fundamental en los sistemas modernos de Big Data:\n",
        "\n",
        "### **Arquitectura Event-Driven (Orientada a Eventos)**\n",
        "\n",
        "Una arquitectura orientada a eventos es un patrón de diseño que promueve la producción, detección, consumo y reacción a eventos. Cuando un evento ocurre, un servicio (o \"productor\") emite una notificación, y otros servicios (o \"consumidores\") que están interesados en ese tipo de evento reaccionan a él. Esto permite una gran flexibilidad y escalabilidad.\n",
        "\n",
        "En una empresa real, esto permite:\n",
        "\n",
        "*   **Automatización:** Los procesos se inician automáticamente sin intervención manual.\n",
        "*   **Escalabilidad:** Los componentes de procesamiento solo se activan cuando hay eventos, escalando según la demanda.\n",
        "*   **Procesamiento Inmediato:** Los datos se procesan tan pronto como llegan, habilitando casos de uso en tiempo real.\n",
        "*   **Desacoplamiento entre componentes:** El sistema que genera el evento (el Data Lake al recibir un archivo) no necesita saber quién lo procesará, y el procesador no necesita saber quién generó el evento. Esto facilita la evolución y el mantenimiento de la arquitectura.\n",
        "\n",
        "**El rol de Eventarc:** Eventarc no transforma los datos. Su función es puramente de **orquestación**: **detecta y dispara** la ejecución de otro componente o servicio en respuesta a un evento."
      ]
    }
  ]
}